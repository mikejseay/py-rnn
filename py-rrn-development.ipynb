{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "from numpy.testing import assert_allclose, assert_array_equal\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "from scipy.stats import norm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib tk\n",
    "\n",
    "import network as N\n",
    "\n",
    "rng_seed = 1234\n",
    "rng = np.random.RandomState(rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new, useful code!\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def assert_allcombs_equal(iter_arr, ck_fun=assert_allclose):\n",
    "    combs = combinations(iter_arr, 2)\n",
    "    for comb in combs:\n",
    "        ck_fun(*comb)\n",
    "\n",
    "def imshow_cb(a, ax):\n",
    "    \n",
    "    i = ax.imshow(a, cmap='RdBu_r')\n",
    "    cb = plt.colorbar(i)\n",
    "\n",
    "    lims = cb.get_clim()\n",
    "    maxabs = np.fabs(lims).max()\n",
    "    cb.set_clim(-maxabs, maxabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'network' from '/export/home/mike/python/py-rrn/network.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GENERATOR_PARAMS = {'n_units': 800,\n",
    "                 'p_plastic': 0.6,\n",
    "                 'p_connect': 0.1,\n",
    "                 'syn_strength': 1.5,\n",
    "                 'tau_ms': 10,\n",
    "                 'sigmoid': np.tanh,\n",
    "                 'noise_amp': 0.001}\n",
    "\n",
    "TRIAL_PARAMS = {'length_ms': 1000,\n",
    "                'spacing': 2,\n",
    "                'time_step': 1,\n",
    "                'start_train_ms': 250,\n",
    "                'end_train_ms': 1400,}\n",
    "\n",
    "INPUT_PARAMS = {'n_units': 1,\n",
    "                'value': 5,\n",
    "                'start_ms': 200,\n",
    "                'duration_ms': 50}\n",
    "\n",
    "OUTPUT_PARAMS = {'n_units': 1,\n",
    "                 'value': 1,\n",
    "                'center_ms': 1250,\n",
    "                'width_ms': 30,\n",
    "                'baseline_val': 0.2}\n",
    "\n",
    "TRAIN_PARAMS = {'n_trials_recurrent': 20,\n",
    "                'n_trials_readout': 10,\n",
    "                'n_trials_test': 10}\n",
    "\n",
    "GEN = N.Network(**GENERATOR_PARAMS)\n",
    "TRYAL = N.Trial(**TRIAL_PARAMS)\n",
    "IN = N.Input(TRYAL, **INPUT_PARAMS)\n",
    "OUT = N.Output(TRYAL, **OUTPUT_PARAMS)\n",
    "TRAIN = N.Trainer(GEN, In, Out, TRYAL, **TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GEN.initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## connectivity matrices\n",
    "\n",
    "# \"generator\" network recurrent weight matrix (WXX)\n",
    "# indices are define as WXX[postsyn, presyn]\n",
    "\n",
    "# logical mask for non-zero connections\n",
    "WXX_mask = np.random.rand(GEN.n_units, GEN.n_units)  # uniform distribution!\n",
    "WXX_mask[WXX_mask <= GEN.p_connect] = 1\n",
    "WXX_mask[WXX_mask < 1] = 0\n",
    "\n",
    "# connection weights\n",
    "WXX_vals = np.random.normal(scale=GEN.scale_recurr, size=(GEN.n_units, GEN.n_units))\n",
    "\n",
    "# create non-sparse version of WXX and set self-connections (diagonal elements) to 0\n",
    "WXX_nonsparse = WXX_vals * WXX_mask\n",
    "np.fill_diagonal(WXX_nonsparse, 0)\n",
    "\n",
    "# convert to be sparse\n",
    "WXX = csr_matrix(WXX_nonsparse)\n",
    "WXX_c = csc_matrix(WXX_nonsparse) # for testing\n",
    "WXX_o = coo_matrix(WXX_nonsparse) # for testing\n",
    "\n",
    "# make a copy\n",
    "WXX_ini = WXX.copy()\n",
    "\n",
    "# input => generator weights\n",
    "WInputX = np.random.normal(scale=1, size=(GEN.n_units, IN.n_units))\n",
    "\n",
    "# generator weights => output\n",
    "WXOut = np.random.normal(scale=1/np.sqrt(GEN.n_units), size=(OUT.n_units, GEN.n_units))\n",
    "\n",
    "# make a copy\n",
    "WXOut_ini = WXOut.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (800, 800) -0.74757972326 0.719404408995\n",
      "<class 'numpy.ndarray'> (800, 1) -3.11938308122 2.99942195034\n",
      "<class 'numpy.ndarray'> (1, 800) -0.101989618004 0.127104372032\n"
     ]
    }
   ],
   "source": [
    "# check type size and range of these mats\n",
    "\n",
    "# ck_mats = (WXX_nonsparse, WXX, WXX_c, WXX_o, WInputX, WXOut)\n",
    "ck_mats = (WXX, WInputX, WXOut)\n",
    "\n",
    "for cm in ck_mats:\n",
    "    print(type(cm), cm.shape, np.min(cm), np.max(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "# trial training time indices\n",
    "\n",
    "print(TRYAL.start_train_n)\n",
    "print(TRYAL.end_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tracking histories\n",
    "\n",
    "# vectors representing the activity of the RRN units and ouputs over time\n",
    "X_history = np.zeros((GEN.n_units, TRYAL.n_steps))\n",
    "Out_history = np.zeros((OUT.n_units, TRYAL.n_steps))\n",
    "\n",
    "# logging changes and such\n",
    "WXOut_len = np.zeros((TRYAL.n_steps))\n",
    "WXX_len = np.zeros((TRYAL.n_steps))\n",
    "dW_readout_len = np.zeros((TRYAL.n_steps))\n",
    "dW_recurr_len = np.zeros((TRYAL.n_steps))\n",
    "train_window = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (800, 1600) 0.0 0.0\n",
      "<class 'numpy.ndarray'> (1, 1600) 0.0 0.0\n",
      "<class 'numpy.ndarray'> (1600,) 0.0 0.0\n",
      "<class 'numpy.ndarray'> (1600,) 0.0 0.0\n",
      "<class 'numpy.ndarray'> (1600,) 0.0 0.0\n",
      "<class 'numpy.ndarray'> (1600,) 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# check type size and range of these mats\n",
    "\n",
    "ck_mats = (X_history, Out_history, WXOut_len, WXX_len, dW_readout_len, dW_recurr_len)\n",
    "\n",
    "for cm in ck_mats:\n",
    "    print(type(cm), cm.shape, np.min(cm), np.max(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial conditions\n",
    "\n",
    "# initial Xv is random uniform distribution from -1 to +1\n",
    "# this represents an analog firing rate\n",
    "Xv = 2 * np.random.rand(GEN.n_units, 1) - 1\n",
    "\n",
    "# X is the sigmoid (tanh) of Xv, which will be bound from -0.76 to +0.76\n",
    "# which represents a membrane potential\n",
    "# as firing rate increases,\n",
    "# membrane potential increases less quickly than linearly\n",
    "X = GEN.sigmoid(Xv)\n",
    "\n",
    "# O represents the output, where each output is the output-weighted membrane potential of each neuron\n",
    "# O is random normal from -0.1 to 0.1\n",
    "O = np.zeros((OUT.n_units,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (800, 1) -0.999089812107 0.994207731077\n",
      "<class 'numpy.ndarray'> (800, 1) -0.761211635339 0.759150800437\n",
      "<class 'numpy.ndarray'> (1, 1) 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# check type size and range of these mats\n",
    "\n",
    "ck_mats = (Xv, X, O)\n",
    "\n",
    "for cm in ck_mats:\n",
    "    print(type(cm), cm.shape, np.min(cm), np.max(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_axes.py:531: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "# what does the sigmoid do?\n",
    "\n",
    "s = np.linspace(-1, 1, 100)\n",
    "h = GEN.sigmoid(s)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "ax.plot(s, h)\n",
    "# ax.plot(s, s, 'k--')\n",
    "ax.set_xlabel('Firing Rate')\n",
    "ax.set_ylabel('Membrane Potential (uV)')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_RECURR = False\n",
    "TRAIN_READOUT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 439/1600 [00:09<00:23, 49.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-22c22d7e571e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mX_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mWXOut_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWXOut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mWXX_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWXX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mikejseay\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mikejseay\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    548\u001b[0m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m            \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m            indptr, indices, data)\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# integration loop\n",
    "\n",
    "# constant value by which the update to Xv based on the summation\n",
    "# of recurrent generator network inputs AND external input inputs\n",
    "# AND noise, are divided...\n",
    "# this simulates a neural time constant?\n",
    "use_noiseamp = 0\n",
    "time_div = GEN.tau_ms / TRYAL.time_step\n",
    "\n",
    "for i in tqdm(range(TRYAL.n_steps)):\n",
    "\n",
    "    # update units\n",
    "    \n",
    "    # (IN.n_units, 1)\n",
    "    in_vec = IN.series[:, i]\n",
    "    \n",
    "    # (GEN.n_units, 1)\n",
    "    noise = use_noiseamp * np.random.normal(scale=np.sqrt(TRYAL.time_step), size=(GEN.n_units,1))\n",
    "    \n",
    "    \n",
    "#     Xv_current = \\\n",
    "#         WXX * X \\ # (GEN.n_units, GEN.n_units) * (GEN.n_units, 1) => (GEN.n_units, 1)\n",
    "#         + \\\n",
    "#         WInputX * in_vec \\ # (GEN.n_units, IN.n_units) * (IN.n_units, 1) => (GEN.n_units, 1)\n",
    "#         + \\\n",
    "#         noise # (GEN.n_units, 1)\n",
    "\n",
    "    # note that X is the \"previous\" membrane potential (random sigmoid vector scaled to .76)\n",
    "    # WXX * X is the weighted previous membrane potential (sparse random normal square matrix scaled to ~0.65 times X)\n",
    "    # WInputX * in_vec is the weighted input (random normal vector scaled to 4 times current input vector/scalar)\n",
    "    Xv_current = WXX * X + WInputX * in_vec + noise\n",
    "    \n",
    "#     Xv += \\  # (GEN.n_units, 1)\n",
    "#         (-Xv + Xv_current) \\ (GEN.n_units, 1) + (GEN.n_units, 1) => (GEN.n_units, 1)\n",
    "#         / \\\n",
    "#         time_div (scalar)\n",
    "    # Xv is previous firing rate\n",
    "    # we take the negative difference between that firing rate and the summed incoming membrane potentials,\n",
    "    # and divide by the time constant of the network (in this case, 10 steps)\n",
    "    # then add that the current firing rate\n",
    "    # the idea being that the change to the current firing rate in each step\n",
    "    # is the summed input of weighted membrane potentials from presynaptic units\n",
    "    Xv += (-Xv + Xv_current) / time_div\n",
    "    \n",
    "    # then we convert that newly updated firing rate back into a sigmoid\n",
    "    # to determine the new membrane potential of all neurons\n",
    "    X = GEN.sigmoid(Xv) # (GEN.n_units, 1) => (GEN.n_units, 1)\n",
    "    \n",
    "    # then we determine all of those neurons outputs as the dot-product of their\n",
    "    # membrane potentials and output weights\n",
    "    O = np.dot(WXOut, X) # (Out.n_units, GEN.n_units) *dot* (GEN.n_units, 1) => (Out.n_units, 1)\n",
    "\n",
    "    # start-end training window\n",
    "    if (i == TRYAL.start_train_n):\n",
    "        train_window = True\n",
    "    if (i == TRYAL.end_train_n):\n",
    "        train_window = False\n",
    "\n",
    "    # training\n",
    "    if train_window and i % TRYAL.spacing == 0:\n",
    "\n",
    "        if TRAIN_RECURR:\n",
    "            # train recurrent\n",
    "            error = X - Target_innate_X[:, i]\n",
    "            for plas in 1:GEN.n_plastic\n",
    "#                 X_pre_plastic = X(pre_plastic_units(plas).inds)\n",
    "#                 P_recurr_old = P_recurr(plas).P\n",
    "#                 P_recurr_old_X = P_recurr_old*X_pre_plastic\n",
    "#                 den_recurr = 1 + X_pre_plastic'*P_recurr_old_X\n",
    "#                 P_recurr(plas).P = P_recurr_old - (P_recurr_old_X*P_recurr_old_X')/den_recurr\n",
    "#                 # update network matrix\n",
    "#                 dW_recurr = -error(plas)*(P_recurr_old_X/den_recurr)'\n",
    "#                 WXX(plas,pre_plastic_units(plas).inds) = WXX(plas,pre_plastic_units(plas).inds) + dW_recurr\n",
    "#                 # store change in weights\n",
    "#                 dW_recurr_len(i) = dW_recurr_len(i) + np.sqrt(dW_recurr*dW_recurr')\n",
    "\n",
    "        if TRAIN_READOUT:\n",
    "            pass\n",
    "            # update inverse correlation matrix (using property P' = P)\n",
    "#             P_readout_old = P_readout\n",
    "#             P_readout_old_X = P_readout_old*X\n",
    "#             den_readout = 1 + X'*P_readout_old_X\n",
    "#             P_readout = P_readout_old - (P_readout_old_X*P_readout_old_X')/den_readout\n",
    "#             # update error\n",
    "#             error = Out - target_Out(i)\n",
    "#             # update output weights\n",
    "#             dW_readout = -error*(P_readout_old_X/den_readout)'\n",
    "#             WXOut = WXOut + dW_readout\n",
    "#             # store change in weights\n",
    "#             dW_readout_len(i) = np.sqrt(dW_readout*dW_readout')\n",
    "\n",
    "    # store output\n",
    "    Out_history[:, i] = O\n",
    "    X_history[:, [i]] = X\n",
    "    WXOut_len[i] = np.sqrt(np.sum(np.square(WXOut[:])))\n",
    "    WXX_len[i] = np.sqrt(np.sum(np.square(WXX[:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bookmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'network' from '/Users/michaelseay/Code/py-rrn/network.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "GENERATOR_PARAMS = {'n_units': 800,\n",
    "                 'p_plastic': 0.6,\n",
    "                 'p_connect': 0.1,\n",
    "                 'syn_strength': 1.5,}\n",
    "\n",
    "TRIAL_PARAMS = {'length_ms': 1000,\n",
    "                'spacing': 2,\n",
    "                'time_step': 1,\n",
    "                'start_train_ms': 250,\n",
    "                'end_train_ms': 1400,}\n",
    "\n",
    "INPUT_PARAMS = {'n_units': 1,\n",
    "                'value': 5,\n",
    "                'start_ms': 200,\n",
    "                'duration_ms': 50}\n",
    "\n",
    "OUTPUT_PARAMS = {'n_units': 1,\n",
    "                 'value': 1,\n",
    "                'center_ms': 1250,\n",
    "                'width_ms': 30,\n",
    "                'baseline_val': 0.2}\n",
    "\n",
    "TRAIN_PARAMS = {'tau_ms': 10,\n",
    "                'sigmoid': np.tanh,\n",
    "                'noise_harvest': 0,\n",
    "                'noise_train': 0.001,\n",
    "                'n_trials_recurrent': 20,\n",
    "                'n_trials_readout': 10,\n",
    "                'n_trials_test': 10,\n",
    "                }\n",
    "\n",
    "# instantiating objects\n",
    "GEN = N.Generator(**GENERATOR_PARAMS)\n",
    "TRYAL = N.Trial(**TRIAL_PARAMS)\n",
    "IN = N.Input(TRYAL, **INPUT_PARAMS)\n",
    "OUT = N.Output(TRYAL, **OUTPUT_PARAMS)\n",
    "TRAIN = N.Trainer(GEN, IN, OUT, TRYAL, **TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN.initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN.harvest_innate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%lprun -f TRAIN.harvest_innate TRAIN.harvest_innate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1513.83it/s]\n"
     ]
    }
   ],
   "source": [
    "%lprun -f main main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "imshow_cb(TRAIN.gen.innate, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage_params = {'innate': {'noise_amp': 0},\n",
    "                'recurrent': {},\n",
    "                'readout': {},\n",
    "                'test': {},\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.] [ 0.]\n",
      "1 [ 0.] [ 0.]\n",
      "2 [ 0.] [ 0.]\n",
      "3 [ 0.] [ 0.]\n",
      "4 [ 0.] [ 0.]\n",
      "5 [ 0.] [ 0.]\n",
      "6 [ 0.] [ 0.]\n",
      "7 [ 0.] [ 0.]\n",
      "8 [ 0.] [ 0.]\n",
      "9 [ 0.] [ 0.]\n",
      "10 [ 0.] [ 0.]\n",
      "11 [ 0.] [ 0.]\n",
      "12 [ 0.] [ 0.]\n",
      "13 [ 0.] [ 0.]\n",
      "14 [ 0.] [ 0.]\n",
      "15 [ 0.] [ 0.]\n",
      "16 [ 0.] [ 0.]\n",
      "17 [ 0.] [ 0.]\n",
      "18 [ 0.] [ 0.]\n",
      "19 [ 0.] [ 0.]\n",
      "20 [ 0.] [ 0.]\n",
      "21 [ 0.] [ 0.]\n",
      "22 [ 0.] [ 0.]\n",
      "23 [ 0.] [ 0.]\n",
      "24 [ 0.] [ 0.]\n",
      "25 [ 0.] [ 0.]\n",
      "26 [ 0.] [ 0.]\n",
      "27 [ 0.] [ 0.]\n",
      "28 [ 0.] [ 0.]\n",
      "29 [ 0.] [ 0.]\n",
      "30 [ 0.] [ 0.]\n",
      "31 [ 0.] [ 0.]\n",
      "32 [ 0.] [ 0.]\n",
      "33 [ 0.] [ 0.]\n",
      "34 [ 0.] [ 0.]\n",
      "35 [ 0.] [ 0.]\n",
      "36 [ 0.] [ 0.]\n",
      "37 [ 0.] [ 0.]\n",
      "38 [ 0.] [ 0.]\n",
      "39 [ 0.] [ 0.]\n",
      "40 [ 0.] [ 0.]\n",
      "41 [ 0.] [ 0.]\n",
      "42 [ 0.] [ 0.]\n",
      "43 [ 0.] [ 0.]\n",
      "44 [ 0.] [ 0.]\n",
      "45 [ 0.] [ 0.]\n",
      "46 [ 0.] [ 0.]\n",
      "47 [ 0.] [ 0.]\n",
      "48 [ 0.] [ 0.]\n",
      "49 [ 0.] [ 0.]\n",
      "50 [ 0.] [ 0.]\n",
      "51 [ 0.] [ 0.]\n",
      "52 [ 0.] [ 0.]\n",
      "53 [ 0.] [ 0.]\n",
      "54 [ 0.] [ 0.]\n",
      "55 [ 0.] [ 0.]\n",
      "56 [ 0.] [ 0.]\n",
      "57 [ 0.] [ 0.]\n",
      "58 [ 0.] [ 0.]\n",
      "59 [ 0.] [ 0.]\n",
      "60 [ 0.] [ 0.]\n",
      "61 [ 0.] [ 0.]\n",
      "62 [ 0.] [ 0.]\n",
      "63 [ 0.] [ 0.]\n",
      "64 [ 0.] [ 0.]\n",
      "65 [ 0.] [ 0.]\n",
      "66 [ 0.] [ 0.]\n",
      "67 [ 0.] [ 0.]\n",
      "68 [ 0.] [ 0.]\n",
      "69 [ 0.] [ 0.]\n",
      "70 [ 0.] [ 0.]\n",
      "71 [ 0.] [ 0.]\n",
      "72 [ 0.] [ 0.]\n",
      "73 [ 0.] [ 0.]\n",
      "74 [ 0.] [ 0.]\n",
      "75 [ 0.] [ 0.]\n",
      "76 [ 0.] [ 0.]\n",
      "77 [ 0.] [ 0.]\n",
      "78 [ 0.] [ 0.]\n",
      "79 [ 0.] [ 0.]\n",
      "80 [ 0.] [ 0.]\n",
      "81 [ 0.] [ 0.]\n",
      "82 [ 0.] [ 0.]\n",
      "83 [ 0.] [ 0.]\n",
      "84 [ 0.] [ 0.]\n",
      "85 [ 0.] [ 0.]\n",
      "86 [ 0.] [ 0.]\n",
      "87 [ 0.] [ 0.]\n",
      "88 [ 0.] [ 0.]\n",
      "89 [ 0.] [ 0.]\n",
      "90 [ 0.] [ 0.]\n",
      "91 [ 0.] [ 0.]\n",
      "92 [ 0.] [ 0.]\n",
      "93 [ 0.] [ 0.]\n",
      "94 [ 0.] [ 0.]\n",
      "95 [ 0.] [ 0.]\n",
      "96 [ 0.] [ 0.]\n",
      "97 [ 0.] [ 0.]\n",
      "98 [ 0.] [ 0.]\n",
      "99 [ 0.] [ 0.]\n",
      "100 [ 0.] [ 0.]\n",
      "101 [ 0.] [ 0.]\n",
      "102 [ 0.] [ 0.]\n",
      "103 [ 0.] [ 0.]\n",
      "104 [ 0.] [ 0.]\n",
      "105 [ 0.] [ 0.]\n",
      "106 [ 0.] [ 0.]\n",
      "107 [ 0.] [ 0.]\n",
      "108 [ 0.] [ 0.]\n",
      "109 [ 0.] [ 0.]\n",
      "110 [ 0.] [ 0.]\n",
      "111 [ 0.] [ 0.]\n",
      "112 [ 0.] [ 0.]\n",
      "113 [ 0.] [ 0.]\n",
      "114 [ 0.] [ 0.]\n",
      "115 [ 0.] [ 0.]\n",
      "116 [ 0.] [ 0.]\n",
      "117 [ 0.] [ 0.]\n",
      "118 [ 0.] [ 0.]\n",
      "119 [ 0.] [ 0.]\n",
      "120 [ 0.] [ 0.]\n",
      "121 [ 0.] [ 0.]\n",
      "122 [ 0.] [ 0.]\n",
      "123 [ 0.] [ 0.]\n",
      "124 [ 0.] [ 0.]\n",
      "125 [ 0.] [ 0.]\n",
      "126 [ 0.] [ 0.]\n",
      "127 [ 0.] [ 0.]\n",
      "128 [ 0.] [ 0.]\n",
      "129 [ 0.] [ 0.]\n",
      "130 [ 0.] [ 0.]\n",
      "131 [ 0.] [ 0.]\n",
      "132 [ 0.] [ 0.]\n",
      "133 [ 0.] [ 0.]\n",
      "134 [ 0.] [ 0.]\n",
      "135 [ 0.] [ 0.]\n",
      "136 [ 0.] [ 0.]\n",
      "137 [ 0.] [ 0.]\n",
      "138 [ 0.] [ 0.]\n",
      "139 [ 0.] [ 0.]\n",
      "140 [ 0.] [ 0.]\n",
      "141 [ 0.] [ 0.]\n",
      "142 [ 0.] [ 0.]\n",
      "143 [ 0.] [ 0.]\n",
      "144 [ 0.] [ 0.]\n",
      "145 [ 0.] [ 0.]\n",
      "146 [ 0.] [ 0.]\n",
      "147 [ 0.] [ 0.]\n",
      "148 [ 0.] [ 0.]\n",
      "149 [ 0.] [ 0.]\n",
      "150 [ 0.] [ 0.]\n",
      "151 [ 0.] [ 0.]\n",
      "152 [ 0.] [ 0.]\n",
      "153 [ 0.] [ 0.]\n",
      "154 [ 0.] [ 0.]\n",
      "155 [ 0.] [ 0.]\n",
      "156 [ 0.] [ 0.]\n",
      "157 [ 0.] [ 0.]\n",
      "158 [ 0.] [ 0.]\n",
      "159 [ 0.] [ 0.]\n",
      "160 [ 0.] [ 0.]\n",
      "161 [ 0.] [ 0.]\n",
      "162 [ 0.] [ 0.]\n",
      "163 [ 0.] [ 0.]\n",
      "164 [ 0.] [ 0.]\n",
      "165 [ 0.] [ 0.]\n",
      "166 [ 0.] [ 0.]\n",
      "167 [ 0.] [ 0.]\n",
      "168 [ 0.] [ 0.]\n",
      "169 [ 0.] [ 0.]\n",
      "170 [ 0.] [ 0.]\n",
      "171 [ 0.] [ 0.]\n",
      "172 [ 0.] [ 0.]\n",
      "173 [ 0.] [ 0.]\n",
      "174 [ 0.] [ 0.]\n",
      "175 [ 0.] [ 0.]\n",
      "176 [ 0.] [ 0.]\n",
      "177 [ 0.] [ 0.]\n",
      "178 [ 0.] [ 0.]\n",
      "179 [ 0.] [ 0.]\n",
      "180 [ 0.] [ 0.]\n",
      "181 [ 0.] [ 0.]\n",
      "182 [ 0.] [ 0.]\n",
      "183 [ 0.] [ 0.]\n",
      "184 [ 0.] [ 0.]\n",
      "185 [ 0.] [ 0.]\n",
      "186 [ 0.] [ 0.]\n",
      "187 [ 0.] [ 0.]\n",
      "188 [ 0.] [ 0.]\n",
      "189 [ 0.] [ 0.]\n",
      "190 [ 0.] [ 0.]\n",
      "191 [ 0.] [ 0.]\n",
      "192 [ 0.] [ 0.]\n",
      "193 [ 0.] [ 0.]\n",
      "194 [ 0.] [ 0.]\n",
      "195 [ 0.] [ 0.]\n",
      "196 [ 0.] [ 0.]\n",
      "197 [ 0.] [ 0.]\n",
      "198 [ 0.] [ 0.]\n",
      "199 [ 0.] [ 0.]\n",
      "200 [ 5.] [ 5.]\n",
      "201 [ 5.] [ 5.]\n",
      "202 [ 5.] [ 5.]\n",
      "203 [ 5.] [ 5.]\n",
      "204 [ 5.] [ 5.]\n",
      "205 [ 5.] [ 5.]\n",
      "206 [ 5.] [ 5.]\n",
      "207 [ 5.] [ 5.]\n",
      "208 [ 5.] [ 5.]\n",
      "209 [ 5.] [ 5.]\n",
      "210 [ 5.] [ 5.]\n",
      "211 [ 5.] [ 5.]\n",
      "212 [ 5.] [ 5.]\n",
      "213 [ 5.] [ 5.]\n",
      "214 [ 5.] [ 5.]\n",
      "215 [ 5.] [ 5.]\n",
      "216 [ 5.] [ 5.]\n",
      "217 [ 5.] [ 5.]\n",
      "218 [ 5.] [ 5.]\n",
      "219 [ 5.] [ 5.]\n",
      "220 [ 5.] [ 5.]\n",
      "221 [ 5.] [ 5.]\n",
      "222 [ 5.] [ 5.]\n",
      "223 [ 5.] [ 5.]\n",
      "224 [ 5.] [ 5.]\n",
      "225 [ 5.] [ 5.]\n",
      "226 [ 5.] [ 5.]\n",
      "227 [ 5.] [ 5.]\n",
      "228 [ 5.] [ 5.]\n",
      "229 [ 5.] [ 5.]\n",
      "230 [ 5.] [ 5.]\n",
      "231 [ 5.] [ 5.]\n",
      "232 [ 5.] [ 5.]\n",
      "233 [ 5.] [ 5.]\n",
      "234 [ 5.] [ 5.]\n",
      "235 [ 5.] [ 5.]\n",
      "236 [ 5.] [ 5.]\n",
      "237 [ 5.] [ 5.]\n",
      "238 [ 5.] [ 5.]\n",
      "239 [ 5.] [ 5.]\n",
      "240 [ 5.] [ 5.]\n",
      "241 [ 5.] [ 5.]\n",
      "242 [ 5.] [ 5.]\n",
      "243 [ 5.] [ 5.]\n",
      "244 [ 5.] [ 5.]\n",
      "245 [ 5.] [ 5.]\n",
      "246 [ 5.] [ 5.]\n",
      "247 [ 5.] [ 5.]\n",
      "248 [ 5.] [ 5.]\n",
      "249 [ 0.] [ 0.]\n",
      "250 [ 0.] [ 0.]\n",
      "251 [ 0.] [ 0.]\n",
      "252 [ 0.] [ 0.]\n",
      "253 [ 0.] [ 0.]\n",
      "254 [ 0.] [ 0.]\n",
      "255 [ 0.] [ 0.]\n",
      "256 [ 0.] [ 0.]\n",
      "257 [ 0.] [ 0.]\n",
      "258 [ 0.] [ 0.]\n",
      "259 [ 0.] [ 0.]\n",
      "260 [ 0.] [ 0.]\n",
      "261 [ 0.] [ 0.]\n",
      "262 [ 0.] [ 0.]\n",
      "263 [ 0.] [ 0.]\n",
      "264 [ 0.] [ 0.]\n",
      "265 [ 0.] [ 0.]\n",
      "266 [ 0.] [ 0.]\n",
      "267 [ 0.] [ 0.]\n",
      "268 [ 0.] [ 0.]\n",
      "269 [ 0.] [ 0.]\n",
      "270 [ 0.] [ 0.]\n",
      "271 [ 0.] [ 0.]\n",
      "272 [ 0.] [ 0.]\n",
      "273 [ 0.] [ 0.]\n",
      "274 [ 0.] [ 0.]\n",
      "275 [ 0.] [ 0.]\n",
      "276 [ 0.] [ 0.]\n",
      "277 [ 0.] [ 0.]\n",
      "278 [ 0.] [ 0.]\n",
      "279 [ 0.] [ 0.]\n",
      "280 [ 0.] [ 0.]\n",
      "281 [ 0.] [ 0.]\n",
      "282 [ 0.] [ 0.]\n",
      "283 [ 0.] [ 0.]\n",
      "284 [ 0.] [ 0.]\n",
      "285 [ 0.] [ 0.]\n",
      "286 [ 0.] [ 0.]\n",
      "287 [ 0.] [ 0.]\n",
      "288 [ 0.] [ 0.]\n",
      "289 [ 0.] [ 0.]\n",
      "290 [ 0.] [ 0.]\n",
      "291 [ 0.] [ 0.]\n",
      "292 [ 0.] [ 0.]\n",
      "293 [ 0.] [ 0.]\n",
      "294 [ 0.] [ 0.]\n",
      "295 [ 0.] [ 0.]\n",
      "296 [ 0.] [ 0.]\n",
      "297 [ 0.] [ 0.]\n",
      "298 [ 0.] [ 0.]\n",
      "299 [ 0.] [ 0.]\n",
      "300 [ 0.] [ 0.]\n",
      "301 [ 0.] [ 0.]\n",
      "302 [ 0.] [ 0.]\n",
      "303 [ 0.] [ 0.]\n",
      "304 [ 0.] [ 0.]\n",
      "305 [ 0.] [ 0.]\n",
      "306 [ 0.] [ 0.]\n",
      "307 [ 0.] [ 0.]\n",
      "308 [ 0.] [ 0.]\n",
      "309 [ 0.] [ 0.]\n",
      "310 [ 0.] [ 0.]\n",
      "311 [ 0.] [ 0.]\n",
      "312 [ 0.] [ 0.]\n",
      "313 [ 0.] [ 0.]\n",
      "314 [ 0.] [ 0.]\n",
      "315 [ 0.] [ 0.]\n",
      "316 [ 0.] [ 0.]\n",
      "317 [ 0.] [ 0.]\n",
      "318 [ 0.] [ 0.]\n",
      "319 [ 0.] [ 0.]\n",
      "320 [ 0.] [ 0.]\n",
      "321 [ 0.] [ 0.]\n",
      "322 [ 0.] [ 0.]\n",
      "323 [ 0.] [ 0.]\n",
      "324 [ 0.] [ 0.]\n",
      "325 [ 0.] [ 0.]\n",
      "326 [ 0.] [ 0.]\n",
      "327 [ 0.] [ 0.]\n",
      "328 [ 0.] [ 0.]\n",
      "329 [ 0.] [ 0.]\n",
      "330 [ 0.] [ 0.]\n",
      "331 [ 0.] [ 0.]\n",
      "332 [ 0.] [ 0.]\n",
      "333 [ 0.] [ 0.]\n",
      "334 [ 0.] [ 0.]\n",
      "335 [ 0.] [ 0.]\n",
      "336 [ 0.] [ 0.]\n",
      "337 [ 0.] [ 0.]\n",
      "338 [ 0.] [ 0.]\n",
      "339 [ 0.] [ 0.]\n",
      "340 [ 0.] [ 0.]\n",
      "341 [ 0.] [ 0.]\n",
      "342 [ 0.] [ 0.]\n",
      "343 [ 0.] [ 0.]\n",
      "344 [ 0.] [ 0.]\n",
      "345 [ 0.] [ 0.]\n",
      "346 [ 0.] [ 0.]\n",
      "347 [ 0.] [ 0.]\n",
      "348 [ 0.] [ 0.]\n",
      "349 [ 0.] [ 0.]\n",
      "350 [ 0.] [ 0.]\n",
      "351 [ 0.] [ 0.]\n",
      "352 [ 0.] [ 0.]\n",
      "353 [ 0.] [ 0.]\n",
      "354 [ 0.] [ 0.]\n",
      "355 [ 0.] [ 0.]\n",
      "356 [ 0.] [ 0.]\n",
      "357 [ 0.] [ 0.]\n",
      "358 [ 0.] [ 0.]\n",
      "359 [ 0.] [ 0.]\n",
      "360 [ 0.] [ 0.]\n",
      "361 [ 0.] [ 0.]\n",
      "362 [ 0.] [ 0.]\n",
      "363 [ 0.] [ 0.]\n",
      "364 [ 0.] [ 0.]\n",
      "365 [ 0.] [ 0.]\n",
      "366 [ 0.] [ 0.]\n",
      "367 [ 0.] [ 0.]\n",
      "368 [ 0.] [ 0.]\n",
      "369 [ 0.] [ 0.]\n",
      "370 [ 0.] [ 0.]\n",
      "371 [ 0.] [ 0.]\n",
      "372 [ 0.] [ 0.]\n",
      "373 [ 0.] [ 0.]\n",
      "374 [ 0.] [ 0.]\n",
      "375 [ 0.] [ 0.]\n",
      "376 [ 0.] [ 0.]\n",
      "377 [ 0.] [ 0.]\n",
      "378 [ 0.] [ 0.]\n",
      "379 [ 0.] [ 0.]\n",
      "380 [ 0.] [ 0.]\n",
      "381 [ 0.] [ 0.]\n",
      "382 [ 0.] [ 0.]\n",
      "383 [ 0.] [ 0.]\n",
      "384 [ 0.] [ 0.]\n",
      "385 [ 0.] [ 0.]\n",
      "386 [ 0.] [ 0.]\n",
      "387 [ 0.] [ 0.]\n",
      "388 [ 0.] [ 0.]\n",
      "389 [ 0.] [ 0.]\n",
      "390 [ 0.] [ 0.]\n",
      "391 [ 0.] [ 0.]\n",
      "392 [ 0.] [ 0.]\n",
      "393 [ 0.] [ 0.]\n",
      "394 [ 0.] [ 0.]\n",
      "395 [ 0.] [ 0.]\n",
      "396 [ 0.] [ 0.]\n",
      "397 [ 0.] [ 0.]\n",
      "398 [ 0.] [ 0.]\n",
      "399 [ 0.] [ 0.]\n",
      "400 [ 0.] [ 0.]\n",
      "401 [ 0.] [ 0.]\n",
      "402 [ 0.] [ 0.]\n",
      "403 [ 0.] [ 0.]\n",
      "404 [ 0.] [ 0.]\n",
      "405 [ 0.] [ 0.]\n",
      "406 [ 0.] [ 0.]\n",
      "407 [ 0.] [ 0.]\n",
      "408 [ 0.] [ 0.]\n",
      "409 [ 0.] [ 0.]\n",
      "410 [ 0.] [ 0.]\n",
      "411 [ 0.] [ 0.]\n",
      "412 [ 0.] [ 0.]\n",
      "413 [ 0.] [ 0.]\n",
      "414 [ 0.] [ 0.]\n",
      "415 [ 0.] [ 0.]\n",
      "416 [ 0.] [ 0.]\n",
      "417 [ 0.] [ 0.]\n",
      "418 [ 0.] [ 0.]\n",
      "419 [ 0.] [ 0.]\n",
      "420 [ 0.] [ 0.]\n",
      "421 [ 0.] [ 0.]\n",
      "422 [ 0.] [ 0.]\n",
      "423 [ 0.] [ 0.]\n",
      "424 [ 0.] [ 0.]\n",
      "425 [ 0.] [ 0.]\n",
      "426 [ 0.] [ 0.]\n",
      "427 [ 0.] [ 0.]\n",
      "428 [ 0.] [ 0.]\n",
      "429 [ 0.] [ 0.]\n",
      "430 [ 0.] [ 0.]\n",
      "431 [ 0.] [ 0.]\n",
      "432 [ 0.] [ 0.]\n",
      "433 [ 0.] [ 0.]\n",
      "434 [ 0.] [ 0.]\n",
      "435 [ 0.] [ 0.]\n",
      "436 [ 0.] [ 0.]\n",
      "437 [ 0.] [ 0.]\n",
      "438 [ 0.] [ 0.]\n",
      "439 [ 0.] [ 0.]\n",
      "440 [ 0.] [ 0.]\n",
      "441 [ 0.] [ 0.]\n",
      "442 [ 0.] [ 0.]\n",
      "443 [ 0.] [ 0.]\n",
      "444 [ 0.] [ 0.]\n",
      "445 [ 0.] [ 0.]\n",
      "446 [ 0.] [ 0.]\n",
      "447 [ 0.] [ 0.]\n",
      "448 [ 0.] [ 0.]\n",
      "449 [ 0.] [ 0.]\n",
      "450 [ 0.] [ 0.]\n",
      "451 [ 0.] [ 0.]\n",
      "452 [ 0.] [ 0.]\n",
      "453 [ 0.] [ 0.]\n",
      "454 [ 0.] [ 0.]\n",
      "455 [ 0.] [ 0.]\n",
      "456 [ 0.] [ 0.]\n",
      "457 [ 0.] [ 0.]\n",
      "458 [ 0.] [ 0.]\n",
      "459 [ 0.] [ 0.]\n",
      "460 [ 0.] [ 0.]\n",
      "461 [ 0.] [ 0.]\n",
      "462 [ 0.] [ 0.]\n",
      "463 [ 0.] [ 0.]\n",
      "464 [ 0.] [ 0.]\n",
      "465 [ 0.] [ 0.]\n",
      "466 [ 0.] [ 0.]\n",
      "467 [ 0.] [ 0.]\n",
      "468 [ 0.] [ 0.]\n",
      "469 [ 0.] [ 0.]\n",
      "470 [ 0.] [ 0.]\n",
      "471 [ 0.] [ 0.]\n",
      "472 [ 0.] [ 0.]\n",
      "473 [ 0.] [ 0.]\n",
      "474 [ 0.] [ 0.]\n",
      "475 [ 0.] [ 0.]\n",
      "476 [ 0.] [ 0.]\n",
      "477 [ 0.] [ 0.]\n",
      "478 [ 0.] [ 0.]\n",
      "479 [ 0.] [ 0.]\n",
      "480 [ 0.] [ 0.]\n",
      "481 [ 0.] [ 0.]\n",
      "482 [ 0.] [ 0.]\n",
      "483 [ 0.] [ 0.]\n",
      "484 [ 0.] [ 0.]\n",
      "485 [ 0.] [ 0.]\n",
      "486 [ 0.] [ 0.]\n",
      "487 [ 0.] [ 0.]\n",
      "488 [ 0.] [ 0.]\n",
      "489 [ 0.] [ 0.]\n",
      "490 [ 0.] [ 0.]\n",
      "491 [ 0.] [ 0.]\n",
      "492 [ 0.] [ 0.]\n",
      "493 [ 0.] [ 0.]\n",
      "494 [ 0.] [ 0.]\n",
      "495 [ 0.] [ 0.]\n",
      "496 [ 0.] [ 0.]\n",
      "497 [ 0.] [ 0.]\n",
      "498 [ 0.] [ 0.]\n",
      "499 [ 0.] [ 0.]\n",
      "500 [ 0.] [ 0.]\n",
      "501 [ 0.] [ 0.]\n",
      "502 [ 0.] [ 0.]\n",
      "503 [ 0.] [ 0.]\n",
      "504 [ 0.] [ 0.]\n",
      "505 [ 0.] [ 0.]\n",
      "506 [ 0.] [ 0.]\n",
      "507 [ 0.] [ 0.]\n",
      "508 [ 0.] [ 0.]\n",
      "509 [ 0.] [ 0.]\n",
      "510 [ 0.] [ 0.]\n",
      "511 [ 0.] [ 0.]\n",
      "512 [ 0.] [ 0.]\n",
      "513 [ 0.] [ 0.]\n",
      "514 [ 0.] [ 0.]\n",
      "515 [ 0.] [ 0.]\n",
      "516 [ 0.] [ 0.]\n",
      "517 [ 0.] [ 0.]\n",
      "518 [ 0.] [ 0.]\n",
      "519 [ 0.] [ 0.]\n",
      "520 [ 0.] [ 0.]\n",
      "521 [ 0.] [ 0.]\n",
      "522 [ 0.] [ 0.]\n",
      "523 [ 0.] [ 0.]\n",
      "524 [ 0.] [ 0.]\n",
      "525 [ 0.] [ 0.]\n",
      "526 [ 0.] [ 0.]\n",
      "527 [ 0.] [ 0.]\n",
      "528 [ 0.] [ 0.]\n",
      "529 [ 0.] [ 0.]\n",
      "530 [ 0.] [ 0.]\n",
      "531 [ 0.] [ 0.]\n",
      "532 [ 0.] [ 0.]\n",
      "533 [ 0.] [ 0.]\n",
      "534 [ 0.] [ 0.]\n",
      "535 [ 0.] [ 0.]\n",
      "536 [ 0.] [ 0.]\n",
      "537 [ 0.] [ 0.]\n",
      "538 [ 0.] [ 0.]\n",
      "539 [ 0.] [ 0.]\n",
      "540 [ 0.] [ 0.]\n",
      "541 [ 0.] [ 0.]\n",
      "542 [ 0.] [ 0.]\n",
      "543 [ 0.] [ 0.]\n",
      "544 [ 0.] [ 0.]\n",
      "545 [ 0.] [ 0.]\n",
      "546 [ 0.] [ 0.]\n",
      "547 [ 0.] [ 0.]\n",
      "548 [ 0.] [ 0.]\n",
      "549 [ 0.] [ 0.]\n",
      "550 [ 0.] [ 0.]\n",
      "551 [ 0.] [ 0.]\n",
      "552 [ 0.] [ 0.]\n",
      "553 [ 0.] [ 0.]\n",
      "554 [ 0.] [ 0.]\n",
      "555 [ 0.] [ 0.]\n",
      "556 [ 0.] [ 0.]\n",
      "557 [ 0.] [ 0.]\n",
      "558 [ 0.] [ 0.]\n",
      "559 [ 0.] [ 0.]\n",
      "560 [ 0.] [ 0.]\n",
      "561 [ 0.] [ 0.]\n",
      "562 [ 0.] [ 0.]\n",
      "563 [ 0.] [ 0.]\n",
      "564 [ 0.] [ 0.]\n",
      "565 [ 0.] [ 0.]\n",
      "566 [ 0.] [ 0.]\n",
      "567 [ 0.] [ 0.]\n",
      "568 [ 0.] [ 0.]\n",
      "569 [ 0.] [ 0.]\n",
      "570 [ 0.] [ 0.]\n",
      "571 [ 0.] [ 0.]\n",
      "572 [ 0.] [ 0.]\n",
      "573 [ 0.] [ 0.]\n",
      "574 [ 0.] [ 0.]\n",
      "575 [ 0.] [ 0.]\n",
      "576 [ 0.] [ 0.]\n",
      "577 [ 0.] [ 0.]\n",
      "578 [ 0.] [ 0.]\n",
      "579 [ 0.] [ 0.]\n",
      "580 [ 0.] [ 0.]\n",
      "581 [ 0.] [ 0.]\n",
      "582 [ 0.] [ 0.]\n",
      "583 [ 0.] [ 0.]\n",
      "584 [ 0.] [ 0.]\n",
      "585 [ 0.] [ 0.]\n",
      "586 [ 0.] [ 0.]\n",
      "587 [ 0.] [ 0.]\n",
      "588 [ 0.] [ 0.]\n",
      "589 [ 0.] [ 0.]\n",
      "590 [ 0.] [ 0.]\n",
      "591 [ 0.] [ 0.]\n",
      "592 [ 0.] [ 0.]\n",
      "593 [ 0.] [ 0.]\n",
      "594 [ 0.] [ 0.]\n",
      "595 [ 0.] [ 0.]\n",
      "596 [ 0.] [ 0.]\n",
      "597 [ 0.] [ 0.]\n",
      "598 [ 0.] [ 0.]\n",
      "599 [ 0.] [ 0.]\n",
      "600 [ 0.] [ 0.]\n",
      "601 [ 0.] [ 0.]\n",
      "602 [ 0.] [ 0.]\n",
      "603 [ 0.] [ 0.]\n",
      "604 [ 0.] [ 0.]\n",
      "605 [ 0.] [ 0.]\n",
      "606 [ 0.] [ 0.]\n",
      "607 [ 0.] [ 0.]\n",
      "608 [ 0.] [ 0.]\n",
      "609 [ 0.] [ 0.]\n",
      "610 [ 0.] [ 0.]\n",
      "611 [ 0.] [ 0.]\n",
      "612 [ 0.] [ 0.]\n",
      "613 [ 0.] [ 0.]\n",
      "614 [ 0.] [ 0.]\n",
      "615 [ 0.] [ 0.]\n",
      "616 [ 0.] [ 0.]\n",
      "617 [ 0.] [ 0.]\n",
      "618 [ 0.] [ 0.]\n",
      "619 [ 0.] [ 0.]\n",
      "620 [ 0.] [ 0.]\n",
      "621 [ 0.] [ 0.]\n",
      "622 [ 0.] [ 0.]\n",
      "623 [ 0.] [ 0.]\n",
      "624 [ 0.] [ 0.]\n",
      "625 [ 0.] [ 0.]\n",
      "626 [ 0.] [ 0.]\n",
      "627 [ 0.] [ 0.]\n",
      "628 [ 0.] [ 0.]\n",
      "629 [ 0.] [ 0.]\n",
      "630 [ 0.] [ 0.]\n",
      "631 [ 0.] [ 0.]\n",
      "632 [ 0.] [ 0.]\n",
      "633 [ 0.] [ 0.]\n",
      "634 [ 0.] [ 0.]\n",
      "635 [ 0.] [ 0.]\n",
      "636 [ 0.] [ 0.]\n",
      "637 [ 0.] [ 0.]\n",
      "638 [ 0.] [ 0.]\n",
      "639 [ 0.] [ 0.]\n",
      "640 [ 0.] [ 0.]\n",
      "641 [ 0.] [ 0.]\n",
      "642 [ 0.] [ 0.]\n",
      "643 [ 0.] [ 0.]\n",
      "644 [ 0.] [ 0.]\n",
      "645 [ 0.] [ 0.]\n",
      "646 [ 0.] [ 0.]\n",
      "647 [ 0.] [ 0.]\n",
      "648 [ 0.] [ 0.]\n",
      "649 [ 0.] [ 0.]\n",
      "650 [ 0.] [ 0.]\n",
      "651 [ 0.] [ 0.]\n",
      "652 [ 0.] [ 0.]\n",
      "653 [ 0.] [ 0.]\n",
      "654 [ 0.] [ 0.]\n",
      "655 [ 0.] [ 0.]\n",
      "656 [ 0.] [ 0.]\n",
      "657 [ 0.] [ 0.]\n",
      "658 [ 0.] [ 0.]\n",
      "659 [ 0.] [ 0.]\n",
      "660 [ 0.] [ 0.]\n",
      "661 [ 0.] [ 0.]\n",
      "662 [ 0.] [ 0.]\n",
      "663 [ 0.] [ 0.]\n",
      "664 [ 0.] [ 0.]\n",
      "665 [ 0.] [ 0.]\n",
      "666 [ 0.] [ 0.]\n",
      "667 [ 0.] [ 0.]\n",
      "668 [ 0.] [ 0.]\n",
      "669 [ 0.] [ 0.]\n",
      "670 [ 0.] [ 0.]\n",
      "671 [ 0.] [ 0.]\n",
      "672 [ 0.] [ 0.]\n",
      "673 [ 0.] [ 0.]\n",
      "674 [ 0.] [ 0.]\n",
      "675 [ 0.] [ 0.]\n",
      "676 [ 0.] [ 0.]\n",
      "677 [ 0.] [ 0.]\n",
      "678 [ 0.] [ 0.]\n",
      "679 [ 0.] [ 0.]\n",
      "680 [ 0.] [ 0.]\n",
      "681 [ 0.] [ 0.]\n",
      "682 [ 0.] [ 0.]\n",
      "683 [ 0.] [ 0.]\n",
      "684 [ 0.] [ 0.]\n",
      "685 [ 0.] [ 0.]\n",
      "686 [ 0.] [ 0.]\n",
      "687 [ 0.] [ 0.]\n",
      "688 [ 0.] [ 0.]\n",
      "689 [ 0.] [ 0.]\n",
      "690 [ 0.] [ 0.]\n",
      "691 [ 0.] [ 0.]\n",
      "692 [ 0.] [ 0.]\n",
      "693 [ 0.] [ 0.]\n",
      "694 [ 0.] [ 0.]\n",
      "695 [ 0.] [ 0.]\n",
      "696 [ 0.] [ 0.]\n",
      "697 [ 0.] [ 0.]\n",
      "698 [ 0.] [ 0.]\n",
      "699 [ 0.] [ 0.]\n",
      "700 [ 0.] [ 0.]\n",
      "701 [ 0.] [ 0.]\n",
      "702 [ 0.] [ 0.]\n",
      "703 [ 0.] [ 0.]\n",
      "704 [ 0.] [ 0.]\n",
      "705 [ 0.] [ 0.]\n",
      "706 [ 0.] [ 0.]\n",
      "707 [ 0.] [ 0.]\n",
      "708 [ 0.] [ 0.]\n",
      "709 [ 0.] [ 0.]\n",
      "710 [ 0.] [ 0.]\n",
      "711 [ 0.] [ 0.]\n",
      "712 [ 0.] [ 0.]\n",
      "713 [ 0.] [ 0.]\n",
      "714 [ 0.] [ 0.]\n",
      "715 [ 0.] [ 0.]\n",
      "716 [ 0.] [ 0.]\n",
      "717 [ 0.] [ 0.]\n",
      "718 [ 0.] [ 0.]\n",
      "719 [ 0.] [ 0.]\n",
      "720 [ 0.] [ 0.]\n",
      "721 [ 0.] [ 0.]\n",
      "722 [ 0.] [ 0.]\n",
      "723 [ 0.] [ 0.]\n",
      "724 [ 0.] [ 0.]\n",
      "725 [ 0.] [ 0.]\n",
      "726 [ 0.] [ 0.]\n",
      "727 [ 0.] [ 0.]\n",
      "728 [ 0.] [ 0.]\n",
      "729 [ 0.] [ 0.]\n",
      "730 [ 0.] [ 0.]\n",
      "731 [ 0.] [ 0.]\n",
      "732 [ 0.] [ 0.]\n",
      "733 [ 0.] [ 0.]\n",
      "734 [ 0.] [ 0.]\n",
      "735 [ 0.] [ 0.]\n",
      "736 [ 0.] [ 0.]\n",
      "737 [ 0.] [ 0.]\n",
      "738 [ 0.] [ 0.]\n",
      "739 [ 0.] [ 0.]\n",
      "740 [ 0.] [ 0.]\n",
      "741 [ 0.] [ 0.]\n",
      "742 [ 0.] [ 0.]\n",
      "743 [ 0.] [ 0.]\n",
      "744 [ 0.] [ 0.]\n",
      "745 [ 0.] [ 0.]\n",
      "746 [ 0.] [ 0.]\n",
      "747 [ 0.] [ 0.]\n",
      "748 [ 0.] [ 0.]\n",
      "749 [ 0.] [ 0.]\n",
      "750 [ 0.] [ 0.]\n",
      "751 [ 0.] [ 0.]\n",
      "752 [ 0.] [ 0.]\n",
      "753 [ 0.] [ 0.]\n",
      "754 [ 0.] [ 0.]\n",
      "755 [ 0.] [ 0.]\n",
      "756 [ 0.] [ 0.]\n",
      "757 [ 0.] [ 0.]\n",
      "758 [ 0.] [ 0.]\n",
      "759 [ 0.] [ 0.]\n",
      "760 [ 0.] [ 0.]\n",
      "761 [ 0.] [ 0.]\n",
      "762 [ 0.] [ 0.]\n",
      "763 [ 0.] [ 0.]\n",
      "764 [ 0.] [ 0.]\n",
      "765 [ 0.] [ 0.]\n",
      "766 [ 0.] [ 0.]\n",
      "767 [ 0.] [ 0.]\n",
      "768 [ 0.] [ 0.]\n",
      "769 [ 0.] [ 0.]\n",
      "770 [ 0.] [ 0.]\n",
      "771 [ 0.] [ 0.]\n",
      "772 [ 0.] [ 0.]\n",
      "773 [ 0.] [ 0.]\n",
      "774 [ 0.] [ 0.]\n",
      "775 [ 0.] [ 0.]\n",
      "776 [ 0.] [ 0.]\n",
      "777 [ 0.] [ 0.]\n",
      "778 [ 0.] [ 0.]\n",
      "779 [ 0.] [ 0.]\n",
      "780 [ 0.] [ 0.]\n",
      "781 [ 0.] [ 0.]\n",
      "782 [ 0.] [ 0.]\n",
      "783 [ 0.] [ 0.]\n",
      "784 [ 0.] [ 0.]\n",
      "785 [ 0.] [ 0.]\n",
      "786 [ 0.] [ 0.]\n",
      "787 [ 0.] [ 0.]\n",
      "788 [ 0.] [ 0.]\n",
      "789 [ 0.] [ 0.]\n",
      "790 [ 0.] [ 0.]\n",
      "791 [ 0.] [ 0.]\n",
      "792 [ 0.] [ 0.]\n",
      "793 [ 0.] [ 0.]\n",
      "794 [ 0.] [ 0.]\n",
      "795 [ 0.] [ 0.]\n",
      "796 [ 0.] [ 0.]\n",
      "797 [ 0.] [ 0.]\n",
      "798 [ 0.] [ 0.]\n",
      "799 [ 0.] [ 0.]\n",
      "800 [ 0.] [ 0.]\n",
      "801 [ 0.] [ 0.]\n",
      "802 [ 0.] [ 0.]\n",
      "803 [ 0.] [ 0.]\n",
      "804 [ 0.] [ 0.]\n",
      "805 [ 0.] [ 0.]\n",
      "806 [ 0.] [ 0.]\n",
      "807 [ 0.] [ 0.]\n",
      "808 [ 0.] [ 0.]\n",
      "809 [ 0.] [ 0.]\n",
      "810 [ 0.] [ 0.]\n",
      "811 [ 0.] [ 0.]\n",
      "812 [ 0.] [ 0.]\n",
      "813 [ 0.] [ 0.]\n",
      "814 [ 0.] [ 0.]\n",
      "815 [ 0.] [ 0.]\n",
      "816 [ 0.] [ 0.]\n",
      "817 [ 0.] [ 0.]\n",
      "818 [ 0.] [ 0.]\n",
      "819 [ 0.] [ 0.]\n",
      "820 [ 0.] [ 0.]\n",
      "821 [ 0.] [ 0.]\n",
      "822 [ 0.] [ 0.]\n",
      "823 [ 0.] [ 0.]\n",
      "824 [ 0.] [ 0.]\n",
      "825 [ 0.] [ 0.]\n",
      "826 [ 0.] [ 0.]\n",
      "827 [ 0.] [ 0.]\n",
      "828 [ 0.] [ 0.]\n",
      "829 [ 0.] [ 0.]\n",
      "830 [ 0.] [ 0.]\n",
      "831 [ 0.] [ 0.]\n",
      "832 [ 0.] [ 0.]\n",
      "833 [ 0.] [ 0.]\n",
      "834 [ 0.] [ 0.]\n",
      "835 [ 0.] [ 0.]\n",
      "836 [ 0.] [ 0.]\n",
      "837 [ 0.] [ 0.]\n",
      "838 [ 0.] [ 0.]\n",
      "839 [ 0.] [ 0.]\n",
      "840 [ 0.] [ 0.]\n",
      "841 [ 0.] [ 0.]\n",
      "842 [ 0.] [ 0.]\n",
      "843 [ 0.] [ 0.]\n",
      "844 [ 0.] [ 0.]\n",
      "845 [ 0.] [ 0.]\n",
      "846 [ 0.] [ 0.]\n",
      "847 [ 0.] [ 0.]\n",
      "848 [ 0.] [ 0.]\n",
      "849 [ 0.] [ 0.]\n",
      "850 [ 0.] [ 0.]\n",
      "851 [ 0.] [ 0.]\n",
      "852 [ 0.] [ 0.]\n",
      "853 [ 0.] [ 0.]\n",
      "854 [ 0.] [ 0.]\n",
      "855 [ 0.] [ 0.]\n",
      "856 [ 0.] [ 0.]\n",
      "857 [ 0.] [ 0.]\n",
      "858 [ 0.] [ 0.]\n",
      "859 [ 0.] [ 0.]\n",
      "860 [ 0.] [ 0.]\n",
      "861 [ 0.] [ 0.]\n",
      "862 [ 0.] [ 0.]\n",
      "863 [ 0.] [ 0.]\n",
      "864 [ 0.] [ 0.]\n",
      "865 [ 0.] [ 0.]\n",
      "866 [ 0.] [ 0.]\n",
      "867 [ 0.] [ 0.]\n",
      "868 [ 0.] [ 0.]\n",
      "869 [ 0.] [ 0.]\n",
      "870 [ 0.] [ 0.]\n",
      "871 [ 0.] [ 0.]\n",
      "872 [ 0.] [ 0.]\n",
      "873 [ 0.] [ 0.]\n",
      "874 [ 0.] [ 0.]\n",
      "875 [ 0.] [ 0.]\n",
      "876 [ 0.] [ 0.]\n",
      "877 [ 0.] [ 0.]\n",
      "878 [ 0.] [ 0.]\n",
      "879 [ 0.] [ 0.]\n",
      "880 [ 0.] [ 0.]\n",
      "881 [ 0.] [ 0.]\n",
      "882 [ 0.] [ 0.]\n",
      "883 [ 0.] [ 0.]\n",
      "884 [ 0.] [ 0.]\n",
      "885 [ 0.] [ 0.]\n",
      "886 [ 0.] [ 0.]\n",
      "887 [ 0.] [ 0.]\n",
      "888 [ 0.] [ 0.]\n",
      "889 [ 0.] [ 0.]\n",
      "890 [ 0.] [ 0.]\n",
      "891 [ 0.] [ 0.]\n",
      "892 [ 0.] [ 0.]\n",
      "893 [ 0.] [ 0.]\n",
      "894 [ 0.] [ 0.]\n",
      "895 [ 0.] [ 0.]\n",
      "896 [ 0.] [ 0.]\n",
      "897 [ 0.] [ 0.]\n",
      "898 [ 0.] [ 0.]\n",
      "899 [ 0.] [ 0.]\n",
      "900 [ 0.] [ 0.]\n",
      "901 [ 0.] [ 0.]\n",
      "902 [ 0.] [ 0.]\n",
      "903 [ 0.] [ 0.]\n",
      "904 [ 0.] [ 0.]\n",
      "905 [ 0.] [ 0.]\n",
      "906 [ 0.] [ 0.]\n",
      "907 [ 0.] [ 0.]\n",
      "908 [ 0.] [ 0.]\n",
      "909 [ 0.] [ 0.]\n",
      "910 [ 0.] [ 0.]\n",
      "911 [ 0.] [ 0.]\n",
      "912 [ 0.] [ 0.]\n",
      "913 [ 0.] [ 0.]\n",
      "914 [ 0.] [ 0.]\n",
      "915 [ 0.] [ 0.]\n",
      "916 [ 0.] [ 0.]\n",
      "917 [ 0.] [ 0.]\n",
      "918 [ 0.] [ 0.]\n",
      "919 [ 0.] [ 0.]\n",
      "920 [ 0.] [ 0.]\n",
      "921 [ 0.] [ 0.]\n",
      "922 [ 0.] [ 0.]\n",
      "923 [ 0.] [ 0.]\n",
      "924 [ 0.] [ 0.]\n",
      "925 [ 0.] [ 0.]\n",
      "926 [ 0.] [ 0.]\n",
      "927 [ 0.] [ 0.]\n",
      "928 [ 0.] [ 0.]\n",
      "929 [ 0.] [ 0.]\n",
      "930 [ 0.] [ 0.]\n",
      "931 [ 0.] [ 0.]\n",
      "932 [ 0.] [ 0.]\n",
      "933 [ 0.] [ 0.]\n",
      "934 [ 0.] [ 0.]\n",
      "935 [ 0.] [ 0.]\n",
      "936 [ 0.] [ 0.]\n",
      "937 [ 0.] [ 0.]\n",
      "938 [ 0.] [ 0.]\n",
      "939 [ 0.] [ 0.]\n",
      "940 [ 0.] [ 0.]\n",
      "941 [ 0.] [ 0.]\n",
      "942 [ 0.] [ 0.]\n",
      "943 [ 0.] [ 0.]\n",
      "944 [ 0.] [ 0.]\n",
      "945 [ 0.] [ 0.]\n",
      "946 [ 0.] [ 0.]\n",
      "947 [ 0.] [ 0.]\n",
      "948 [ 0.] [ 0.]\n",
      "949 [ 0.] [ 0.]\n",
      "950 [ 0.] [ 0.]\n",
      "951 [ 0.] [ 0.]\n",
      "952 [ 0.] [ 0.]\n",
      "953 [ 0.] [ 0.]\n",
      "954 [ 0.] [ 0.]\n",
      "955 [ 0.] [ 0.]\n",
      "956 [ 0.] [ 0.]\n",
      "957 [ 0.] [ 0.]\n",
      "958 [ 0.] [ 0.]\n",
      "959 [ 0.] [ 0.]\n",
      "960 [ 0.] [ 0.]\n",
      "961 [ 0.] [ 0.]\n",
      "962 [ 0.] [ 0.]\n",
      "963 [ 0.] [ 0.]\n",
      "964 [ 0.] [ 0.]\n",
      "965 [ 0.] [ 0.]\n",
      "966 [ 0.] [ 0.]\n",
      "967 [ 0.] [ 0.]\n",
      "968 [ 0.] [ 0.]\n",
      "969 [ 0.] [ 0.]\n",
      "970 [ 0.] [ 0.]\n",
      "971 [ 0.] [ 0.]\n",
      "972 [ 0.] [ 0.]\n",
      "973 [ 0.] [ 0.]\n",
      "974 [ 0.] [ 0.]\n",
      "975 [ 0.] [ 0.]\n",
      "976 [ 0.] [ 0.]\n",
      "977 [ 0.] [ 0.]\n",
      "978 [ 0.] [ 0.]\n",
      "979 [ 0.] [ 0.]\n",
      "980 [ 0.] [ 0.]\n",
      "981 [ 0.] [ 0.]\n",
      "982 [ 0.] [ 0.]\n",
      "983 [ 0.] [ 0.]\n",
      "984 [ 0.] [ 0.]\n",
      "985 [ 0.] [ 0.]\n",
      "986 [ 0.] [ 0.]\n",
      "987 [ 0.] [ 0.]\n",
      "988 [ 0.] [ 0.]\n",
      "989 [ 0.] [ 0.]\n",
      "990 [ 0.] [ 0.]\n",
      "991 [ 0.] [ 0.]\n",
      "992 [ 0.] [ 0.]\n",
      "993 [ 0.] [ 0.]\n",
      "994 [ 0.] [ 0.]\n",
      "995 [ 0.] [ 0.]\n",
      "996 [ 0.] [ 0.]\n",
      "997 [ 0.] [ 0.]\n",
      "998 [ 0.] [ 0.]\n",
      "999 [ 0.] [ 0.]\n",
      "1000 [ 0.] [ 0.]\n",
      "1001 [ 0.] [ 0.]\n",
      "1002 [ 0.] [ 0.]\n",
      "1003 [ 0.] [ 0.]\n",
      "1004 [ 0.] [ 0.]\n",
      "1005 [ 0.] [ 0.]\n",
      "1006 [ 0.] [ 0.]\n",
      "1007 [ 0.] [ 0.]\n",
      "1008 [ 0.] [ 0.]\n",
      "1009 [ 0.] [ 0.]\n",
      "1010 [ 0.] [ 0.]\n",
      "1011 [ 0.] [ 0.]\n",
      "1012 [ 0.] [ 0.]\n",
      "1013 [ 0.] [ 0.]\n",
      "1014 [ 0.] [ 0.]\n",
      "1015 [ 0.] [ 0.]\n",
      "1016 [ 0.] [ 0.]\n",
      "1017 [ 0.] [ 0.]\n",
      "1018 [ 0.] [ 0.]\n",
      "1019 [ 0.] [ 0.]\n",
      "1020 [ 0.] [ 0.]\n",
      "1021 [ 0.] [ 0.]\n",
      "1022 [ 0.] [ 0.]\n",
      "1023 [ 0.] [ 0.]\n",
      "1024 [ 0.] [ 0.]\n",
      "1025 [ 0.] [ 0.]\n",
      "1026 [ 0.] [ 0.]\n",
      "1027 [ 0.] [ 0.]\n",
      "1028 [ 0.] [ 0.]\n",
      "1029 [ 0.] [ 0.]\n",
      "1030 [ 0.] [ 0.]\n",
      "1031 [ 0.] [ 0.]\n",
      "1032 [ 0.] [ 0.]\n",
      "1033 [ 0.] [ 0.]\n",
      "1034 [ 0.] [ 0.]\n",
      "1035 [ 0.] [ 0.]\n",
      "1036 [ 0.] [ 0.]\n",
      "1037 [ 0.] [ 0.]\n",
      "1038 [ 0.] [ 0.]\n",
      "1039 [ 0.] [ 0.]\n",
      "1040 [ 0.] [ 0.]\n",
      "1041 [ 0.] [ 0.]\n",
      "1042 [ 0.] [ 0.]\n",
      "1043 [ 0.] [ 0.]\n",
      "1044 [ 0.] [ 0.]\n",
      "1045 [ 0.] [ 0.]\n",
      "1046 [ 0.] [ 0.]\n",
      "1047 [ 0.] [ 0.]\n",
      "1048 [ 0.] [ 0.]\n",
      "1049 [ 0.] [ 0.]\n",
      "1050 [ 0.] [ 0.]\n",
      "1051 [ 0.] [ 0.]\n",
      "1052 [ 0.] [ 0.]\n",
      "1053 [ 0.] [ 0.]\n",
      "1054 [ 0.] [ 0.]\n",
      "1055 [ 0.] [ 0.]\n",
      "1056 [ 0.] [ 0.]\n",
      "1057 [ 0.] [ 0.]\n",
      "1058 [ 0.] [ 0.]\n",
      "1059 [ 0.] [ 0.]\n",
      "1060 [ 0.] [ 0.]\n",
      "1061 [ 0.] [ 0.]\n",
      "1062 [ 0.] [ 0.]\n",
      "1063 [ 0.] [ 0.]\n",
      "1064 [ 0.] [ 0.]\n",
      "1065 [ 0.] [ 0.]\n",
      "1066 [ 0.] [ 0.]\n",
      "1067 [ 0.] [ 0.]\n",
      "1068 [ 0.] [ 0.]\n",
      "1069 [ 0.] [ 0.]\n",
      "1070 [ 0.] [ 0.]\n",
      "1071 [ 0.] [ 0.]\n",
      "1072 [ 0.] [ 0.]\n",
      "1073 [ 0.] [ 0.]\n",
      "1074 [ 0.] [ 0.]\n",
      "1075 [ 0.] [ 0.]\n",
      "1076 [ 0.] [ 0.]\n",
      "1077 [ 0.] [ 0.]\n",
      "1078 [ 0.] [ 0.]\n",
      "1079 [ 0.] [ 0.]\n",
      "1080 [ 0.] [ 0.]\n",
      "1081 [ 0.] [ 0.]\n",
      "1082 [ 0.] [ 0.]\n",
      "1083 [ 0.] [ 0.]\n",
      "1084 [ 0.] [ 0.]\n",
      "1085 [ 0.] [ 0.]\n",
      "1086 [ 0.] [ 0.]\n",
      "1087 [ 0.] [ 0.]\n",
      "1088 [ 0.] [ 0.]\n",
      "1089 [ 0.] [ 0.]\n",
      "1090 [ 0.] [ 0.]\n",
      "1091 [ 0.] [ 0.]\n",
      "1092 [ 0.] [ 0.]\n",
      "1093 [ 0.] [ 0.]\n",
      "1094 [ 0.] [ 0.]\n",
      "1095 [ 0.] [ 0.]\n",
      "1096 [ 0.] [ 0.]\n",
      "1097 [ 0.] [ 0.]\n",
      "1098 [ 0.] [ 0.]\n",
      "1099 [ 0.] [ 0.]\n",
      "1100 [ 0.] [ 0.]\n",
      "1101 [ 0.] [ 0.]\n",
      "1102 [ 0.] [ 0.]\n",
      "1103 [ 0.] [ 0.]\n",
      "1104 [ 0.] [ 0.]\n",
      "1105 [ 0.] [ 0.]\n",
      "1106 [ 0.] [ 0.]\n",
      "1107 [ 0.] [ 0.]\n",
      "1108 [ 0.] [ 0.]\n",
      "1109 [ 0.] [ 0.]\n",
      "1110 [ 0.] [ 0.]\n",
      "1111 [ 0.] [ 0.]\n",
      "1112 [ 0.] [ 0.]\n",
      "1113 [ 0.] [ 0.]\n",
      "1114 [ 0.] [ 0.]\n",
      "1115 [ 0.] [ 0.]\n",
      "1116 [ 0.] [ 0.]\n",
      "1117 [ 0.] [ 0.]\n",
      "1118 [ 0.] [ 0.]\n",
      "1119 [ 0.] [ 0.]\n",
      "1120 [ 0.] [ 0.]\n",
      "1121 [ 0.] [ 0.]\n",
      "1122 [ 0.] [ 0.]\n",
      "1123 [ 0.] [ 0.]\n",
      "1124 [ 0.] [ 0.]\n",
      "1125 [ 0.] [ 0.]\n",
      "1126 [ 0.] [ 0.]\n",
      "1127 [ 0.] [ 0.]\n",
      "1128 [ 0.] [ 0.]\n",
      "1129 [ 0.] [ 0.]\n",
      "1130 [ 0.] [ 0.]\n",
      "1131 [ 0.] [ 0.]\n",
      "1132 [ 0.] [ 0.]\n",
      "1133 [ 0.] [ 0.]\n",
      "1134 [ 0.] [ 0.]\n",
      "1135 [ 0.] [ 0.]\n",
      "1136 [ 0.] [ 0.]\n",
      "1137 [ 0.] [ 0.]\n",
      "1138 [ 0.] [ 0.]\n",
      "1139 [ 0.] [ 0.]\n",
      "1140 [ 0.] [ 0.]\n",
      "1141 [ 0.] [ 0.]\n",
      "1142 [ 0.] [ 0.]\n",
      "1143 [ 0.] [ 0.]\n",
      "1144 [ 0.] [ 0.]\n",
      "1145 [ 0.] [ 0.]\n",
      "1146 [ 0.] [ 0.]\n",
      "1147 [ 0.] [ 0.]\n",
      "1148 [ 0.] [ 0.]\n",
      "1149 [ 0.] [ 0.]\n",
      "1150 [ 0.] [ 0.]\n",
      "1151 [ 0.] [ 0.]\n",
      "1152 [ 0.] [ 0.]\n",
      "1153 [ 0.] [ 0.]\n",
      "1154 [ 0.] [ 0.]\n",
      "1155 [ 0.] [ 0.]\n",
      "1156 [ 0.] [ 0.]\n",
      "1157 [ 0.] [ 0.]\n",
      "1158 [ 0.] [ 0.]\n",
      "1159 [ 0.] [ 0.]\n",
      "1160 [ 0.] [ 0.]\n",
      "1161 [ 0.] [ 0.]\n",
      "1162 [ 0.] [ 0.]\n",
      "1163 [ 0.] [ 0.]\n",
      "1164 [ 0.] [ 0.]\n",
      "1165 [ 0.] [ 0.]\n",
      "1166 [ 0.] [ 0.]\n",
      "1167 [ 0.] [ 0.]\n",
      "1168 [ 0.] [ 0.]\n",
      "1169 [ 0.] [ 0.]\n",
      "1170 [ 0.] [ 0.]\n",
      "1171 [ 0.] [ 0.]\n",
      "1172 [ 0.] [ 0.]\n",
      "1173 [ 0.] [ 0.]\n",
      "1174 [ 0.] [ 0.]\n",
      "1175 [ 0.] [ 0.]\n",
      "1176 [ 0.] [ 0.]\n",
      "1177 [ 0.] [ 0.]\n",
      "1178 [ 0.] [ 0.]\n",
      "1179 [ 0.] [ 0.]\n",
      "1180 [ 0.] [ 0.]\n",
      "1181 [ 0.] [ 0.]\n",
      "1182 [ 0.] [ 0.]\n",
      "1183 [ 0.] [ 0.]\n",
      "1184 [ 0.] [ 0.]\n",
      "1185 [ 0.] [ 0.]\n",
      "1186 [ 0.] [ 0.]\n",
      "1187 [ 0.] [ 0.]\n",
      "1188 [ 0.] [ 0.]\n",
      "1189 [ 0.] [ 0.]\n",
      "1190 [ 0.] [ 0.]\n",
      "1191 [ 0.] [ 0.]\n",
      "1192 [ 0.] [ 0.]\n",
      "1193 [ 0.] [ 0.]\n",
      "1194 [ 0.] [ 0.]\n",
      "1195 [ 0.] [ 0.]\n",
      "1196 [ 0.] [ 0.]\n",
      "1197 [ 0.] [ 0.]\n",
      "1198 [ 0.] [ 0.]\n",
      "1199 [ 0.] [ 0.]\n",
      "1200 [ 0.] [ 0.]\n",
      "1201 [ 0.] [ 0.]\n",
      "1202 [ 0.] [ 0.]\n",
      "1203 [ 0.] [ 0.]\n",
      "1204 [ 0.] [ 0.]\n",
      "1205 [ 0.] [ 0.]\n",
      "1206 [ 0.] [ 0.]\n",
      "1207 [ 0.] [ 0.]\n",
      "1208 [ 0.] [ 0.]\n",
      "1209 [ 0.] [ 0.]\n",
      "1210 [ 0.] [ 0.]\n",
      "1211 [ 0.] [ 0.]\n",
      "1212 [ 0.] [ 0.]\n",
      "1213 [ 0.] [ 0.]\n",
      "1214 [ 0.] [ 0.]\n",
      "1215 [ 0.] [ 0.]\n",
      "1216 [ 0.] [ 0.]\n",
      "1217 [ 0.] [ 0.]\n",
      "1218 [ 0.] [ 0.]\n",
      "1219 [ 0.] [ 0.]\n",
      "1220 [ 0.] [ 0.]\n",
      "1221 [ 0.] [ 0.]\n",
      "1222 [ 0.] [ 0.]\n",
      "1223 [ 0.] [ 0.]\n",
      "1224 [ 0.] [ 0.]\n",
      "1225 [ 0.] [ 0.]\n",
      "1226 [ 0.] [ 0.]\n",
      "1227 [ 0.] [ 0.]\n",
      "1228 [ 0.] [ 0.]\n",
      "1229 [ 0.] [ 0.]\n",
      "1230 [ 0.] [ 0.]\n",
      "1231 [ 0.] [ 0.]\n",
      "1232 [ 0.] [ 0.]\n",
      "1233 [ 0.] [ 0.]\n",
      "1234 [ 0.] [ 0.]\n",
      "1235 [ 0.] [ 0.]\n",
      "1236 [ 0.] [ 0.]\n",
      "1237 [ 0.] [ 0.]\n",
      "1238 [ 0.] [ 0.]\n",
      "1239 [ 0.] [ 0.]\n",
      "1240 [ 0.] [ 0.]\n",
      "1241 [ 0.] [ 0.]\n",
      "1242 [ 0.] [ 0.]\n",
      "1243 [ 0.] [ 0.]\n",
      "1244 [ 0.] [ 0.]\n",
      "1245 [ 0.] [ 0.]\n",
      "1246 [ 0.] [ 0.]\n",
      "1247 [ 0.] [ 0.]\n",
      "1248 [ 0.] [ 0.]\n",
      "1249 [ 0.] [ 0.]\n",
      "1250 [ 0.] [ 0.]\n",
      "1251 [ 0.] [ 0.]\n",
      "1252 [ 0.] [ 0.]\n",
      "1253 [ 0.] [ 0.]\n",
      "1254 [ 0.] [ 0.]\n",
      "1255 [ 0.] [ 0.]\n",
      "1256 [ 0.] [ 0.]\n",
      "1257 [ 0.] [ 0.]\n",
      "1258 [ 0.] [ 0.]\n",
      "1259 [ 0.] [ 0.]\n",
      "1260 [ 0.] [ 0.]\n",
      "1261 [ 0.] [ 0.]\n",
      "1262 [ 0.] [ 0.]\n",
      "1263 [ 0.] [ 0.]\n",
      "1264 [ 0.] [ 0.]\n",
      "1265 [ 0.] [ 0.]\n",
      "1266 [ 0.] [ 0.]\n",
      "1267 [ 0.] [ 0.]\n",
      "1268 [ 0.] [ 0.]\n",
      "1269 [ 0.] [ 0.]\n",
      "1270 [ 0.] [ 0.]\n",
      "1271 [ 0.] [ 0.]\n",
      "1272 [ 0.] [ 0.]\n",
      "1273 [ 0.] [ 0.]\n",
      "1274 [ 0.] [ 0.]\n",
      "1275 [ 0.] [ 0.]\n",
      "1276 [ 0.] [ 0.]\n",
      "1277 [ 0.] [ 0.]\n",
      "1278 [ 0.] [ 0.]\n",
      "1279 [ 0.] [ 0.]\n",
      "1280 [ 0.] [ 0.]\n",
      "1281 [ 0.] [ 0.]\n",
      "1282 [ 0.] [ 0.]\n",
      "1283 [ 0.] [ 0.]\n",
      "1284 [ 0.] [ 0.]\n",
      "1285 [ 0.] [ 0.]\n",
      "1286 [ 0.] [ 0.]\n",
      "1287 [ 0.] [ 0.]\n",
      "1288 [ 0.] [ 0.]\n",
      "1289 [ 0.] [ 0.]\n",
      "1290 [ 0.] [ 0.]\n",
      "1291 [ 0.] [ 0.]\n",
      "1292 [ 0.] [ 0.]\n",
      "1293 [ 0.] [ 0.]\n",
      "1294 [ 0.] [ 0.]\n",
      "1295 [ 0.] [ 0.]\n",
      "1296 [ 0.] [ 0.]\n",
      "1297 [ 0.] [ 0.]\n",
      "1298 [ 0.] [ 0.]\n",
      "1299 [ 0.] [ 0.]\n",
      "1300 [ 0.] [ 0.]\n",
      "1301 [ 0.] [ 0.]\n",
      "1302 [ 0.] [ 0.]\n",
      "1303 [ 0.] [ 0.]\n",
      "1304 [ 0.] [ 0.]\n",
      "1305 [ 0.] [ 0.]\n",
      "1306 [ 0.] [ 0.]\n",
      "1307 [ 0.] [ 0.]\n",
      "1308 [ 0.] [ 0.]\n",
      "1309 [ 0.] [ 0.]\n",
      "1310 [ 0.] [ 0.]\n",
      "1311 [ 0.] [ 0.]\n",
      "1312 [ 0.] [ 0.]\n",
      "1313 [ 0.] [ 0.]\n",
      "1314 [ 0.] [ 0.]\n",
      "1315 [ 0.] [ 0.]\n",
      "1316 [ 0.] [ 0.]\n",
      "1317 [ 0.] [ 0.]\n",
      "1318 [ 0.] [ 0.]\n",
      "1319 [ 0.] [ 0.]\n",
      "1320 [ 0.] [ 0.]\n",
      "1321 [ 0.] [ 0.]\n",
      "1322 [ 0.] [ 0.]\n",
      "1323 [ 0.] [ 0.]\n",
      "1324 [ 0.] [ 0.]\n",
      "1325 [ 0.] [ 0.]\n",
      "1326 [ 0.] [ 0.]\n",
      "1327 [ 0.] [ 0.]\n",
      "1328 [ 0.] [ 0.]\n",
      "1329 [ 0.] [ 0.]\n",
      "1330 [ 0.] [ 0.]\n",
      "1331 [ 0.] [ 0.]\n",
      "1332 [ 0.] [ 0.]\n",
      "1333 [ 0.] [ 0.]\n",
      "1334 [ 0.] [ 0.]\n",
      "1335 [ 0.] [ 0.]\n",
      "1336 [ 0.] [ 0.]\n",
      "1337 [ 0.] [ 0.]\n",
      "1338 [ 0.] [ 0.]\n",
      "1339 [ 0.] [ 0.]\n",
      "1340 [ 0.] [ 0.]\n",
      "1341 [ 0.] [ 0.]\n",
      "1342 [ 0.] [ 0.]\n",
      "1343 [ 0.] [ 0.]\n",
      "1344 [ 0.] [ 0.]\n",
      "1345 [ 0.] [ 0.]\n",
      "1346 [ 0.] [ 0.]\n",
      "1347 [ 0.] [ 0.]\n",
      "1348 [ 0.] [ 0.]\n",
      "1349 [ 0.] [ 0.]\n",
      "1350 [ 0.] [ 0.]\n",
      "1351 [ 0.] [ 0.]\n",
      "1352 [ 0.] [ 0.]\n",
      "1353 [ 0.] [ 0.]\n",
      "1354 [ 0.] [ 0.]\n",
      "1355 [ 0.] [ 0.]\n",
      "1356 [ 0.] [ 0.]\n",
      "1357 [ 0.] [ 0.]\n",
      "1358 [ 0.] [ 0.]\n",
      "1359 [ 0.] [ 0.]\n",
      "1360 [ 0.] [ 0.]\n",
      "1361 [ 0.] [ 0.]\n",
      "1362 [ 0.] [ 0.]\n",
      "1363 [ 0.] [ 0.]\n",
      "1364 [ 0.] [ 0.]\n",
      "1365 [ 0.] [ 0.]\n",
      "1366 [ 0.] [ 0.]\n",
      "1367 [ 0.] [ 0.]\n",
      "1368 [ 0.] [ 0.]\n",
      "1369 [ 0.] [ 0.]\n",
      "1370 [ 0.] [ 0.]\n",
      "1371 [ 0.] [ 0.]\n",
      "1372 [ 0.] [ 0.]\n",
      "1373 [ 0.] [ 0.]\n",
      "1374 [ 0.] [ 0.]\n",
      "1375 [ 0.] [ 0.]\n",
      "1376 [ 0.] [ 0.]\n",
      "1377 [ 0.] [ 0.]\n",
      "1378 [ 0.] [ 0.]\n",
      "1379 [ 0.] [ 0.]\n",
      "1380 [ 0.] [ 0.]\n",
      "1381 [ 0.] [ 0.]\n",
      "1382 [ 0.] [ 0.]\n",
      "1383 [ 0.] [ 0.]\n",
      "1384 [ 0.] [ 0.]\n",
      "1385 [ 0.] [ 0.]\n",
      "1386 [ 0.] [ 0.]\n",
      "1387 [ 0.] [ 0.]\n",
      "1388 [ 0.] [ 0.]\n",
      "1389 [ 0.] [ 0.]\n",
      "1390 [ 0.] [ 0.]\n",
      "1391 [ 0.] [ 0.]\n",
      "1392 [ 0.] [ 0.]\n",
      "1393 [ 0.] [ 0.]\n",
      "1394 [ 0.] [ 0.]\n",
      "1395 [ 0.] [ 0.]\n",
      "1396 [ 0.] [ 0.]\n",
      "1397 [ 0.] [ 0.]\n",
      "1398 [ 0.] [ 0.]\n",
      "1399 [ 0.] [ 0.]\n",
      "1400 [ 0.] [ 0.]\n",
      "1401 [ 0.] [ 0.]\n",
      "1402 [ 0.] [ 0.]\n",
      "1403 [ 0.] [ 0.]\n",
      "1404 [ 0.] [ 0.]\n",
      "1405 [ 0.] [ 0.]\n",
      "1406 [ 0.] [ 0.]\n",
      "1407 [ 0.] [ 0.]\n",
      "1408 [ 0.] [ 0.]\n",
      "1409 [ 0.] [ 0.]\n",
      "1410 [ 0.] [ 0.]\n",
      "1411 [ 0.] [ 0.]\n",
      "1412 [ 0.] [ 0.]\n",
      "1413 [ 0.] [ 0.]\n",
      "1414 [ 0.] [ 0.]\n",
      "1415 [ 0.] [ 0.]\n",
      "1416 [ 0.] [ 0.]\n",
      "1417 [ 0.] [ 0.]\n",
      "1418 [ 0.] [ 0.]\n",
      "1419 [ 0.] [ 0.]\n",
      "1420 [ 0.] [ 0.]\n",
      "1421 [ 0.] [ 0.]\n",
      "1422 [ 0.] [ 0.]\n",
      "1423 [ 0.] [ 0.]\n",
      "1424 [ 0.] [ 0.]\n",
      "1425 [ 0.] [ 0.]\n",
      "1426 [ 0.] [ 0.]\n",
      "1427 [ 0.] [ 0.]\n",
      "1428 [ 0.] [ 0.]\n",
      "1429 [ 0.] [ 0.]\n",
      "1430 [ 0.] [ 0.]\n",
      "1431 [ 0.] [ 0.]\n",
      "1432 [ 0.] [ 0.]\n",
      "1433 [ 0.] [ 0.]\n",
      "1434 [ 0.] [ 0.]\n",
      "1435 [ 0.] [ 0.]\n",
      "1436 [ 0.] [ 0.]\n",
      "1437 [ 0.] [ 0.]\n",
      "1438 [ 0.] [ 0.]\n",
      "1439 [ 0.] [ 0.]\n",
      "1440 [ 0.] [ 0.]\n",
      "1441 [ 0.] [ 0.]\n",
      "1442 [ 0.] [ 0.]\n",
      "1443 [ 0.] [ 0.]\n",
      "1444 [ 0.] [ 0.]\n",
      "1445 [ 0.] [ 0.]\n",
      "1446 [ 0.] [ 0.]\n",
      "1447 [ 0.] [ 0.]\n",
      "1448 [ 0.] [ 0.]\n",
      "1449 [ 0.] [ 0.]\n",
      "1450 [ 0.] [ 0.]\n",
      "1451 [ 0.] [ 0.]\n",
      "1452 [ 0.] [ 0.]\n",
      "1453 [ 0.] [ 0.]\n",
      "1454 [ 0.] [ 0.]\n",
      "1455 [ 0.] [ 0.]\n",
      "1456 [ 0.] [ 0.]\n",
      "1457 [ 0.] [ 0.]\n",
      "1458 [ 0.] [ 0.]\n",
      "1459 [ 0.] [ 0.]\n",
      "1460 [ 0.] [ 0.]\n",
      "1461 [ 0.] [ 0.]\n",
      "1462 [ 0.] [ 0.]\n",
      "1463 [ 0.] [ 0.]\n",
      "1464 [ 0.] [ 0.]\n",
      "1465 [ 0.] [ 0.]\n",
      "1466 [ 0.] [ 0.]\n",
      "1467 [ 0.] [ 0.]\n",
      "1468 [ 0.] [ 0.]\n",
      "1469 [ 0.] [ 0.]\n",
      "1470 [ 0.] [ 0.]\n",
      "1471 [ 0.] [ 0.]\n",
      "1472 [ 0.] [ 0.]\n",
      "1473 [ 0.] [ 0.]\n",
      "1474 [ 0.] [ 0.]\n",
      "1475 [ 0.] [ 0.]\n",
      "1476 [ 0.] [ 0.]\n",
      "1477 [ 0.] [ 0.]\n",
      "1478 [ 0.] [ 0.]\n",
      "1479 [ 0.] [ 0.]\n",
      "1480 [ 0.] [ 0.]\n",
      "1481 [ 0.] [ 0.]\n",
      "1482 [ 0.] [ 0.]\n",
      "1483 [ 0.] [ 0.]\n",
      "1484 [ 0.] [ 0.]\n",
      "1485 [ 0.] [ 0.]\n",
      "1486 [ 0.] [ 0.]\n",
      "1487 [ 0.] [ 0.]\n",
      "1488 [ 0.] [ 0.]\n",
      "1489 [ 0.] [ 0.]\n",
      "1490 [ 0.] [ 0.]\n",
      "1491 [ 0.] [ 0.]\n",
      "1492 [ 0.] [ 0.]\n",
      "1493 [ 0.] [ 0.]\n",
      "1494 [ 0.] [ 0.]\n",
      "1495 [ 0.] [ 0.]\n",
      "1496 [ 0.] [ 0.]\n",
      "1497 [ 0.] [ 0.]\n",
      "1498 [ 0.] [ 0.]\n",
      "1499 [ 0.] [ 0.]\n",
      "1500 [ 0.] [ 0.]\n",
      "1501 [ 0.] [ 0.]\n",
      "1502 [ 0.] [ 0.]\n",
      "1503 [ 0.] [ 0.]\n",
      "1504 [ 0.] [ 0.]\n",
      "1505 [ 0.] [ 0.]\n",
      "1506 [ 0.] [ 0.]\n",
      "1507 [ 0.] [ 0.]\n",
      "1508 [ 0.] [ 0.]\n",
      "1509 [ 0.] [ 0.]\n",
      "1510 [ 0.] [ 0.]\n",
      "1511 [ 0.] [ 0.]\n",
      "1512 [ 0.] [ 0.]\n",
      "1513 [ 0.] [ 0.]\n",
      "1514 [ 0.] [ 0.]\n",
      "1515 [ 0.] [ 0.]\n",
      "1516 [ 0.] [ 0.]\n",
      "1517 [ 0.] [ 0.]\n",
      "1518 [ 0.] [ 0.]\n",
      "1519 [ 0.] [ 0.]\n",
      "1520 [ 0.] [ 0.]\n",
      "1521 [ 0.] [ 0.]\n",
      "1522 [ 0.] [ 0.]\n",
      "1523 [ 0.] [ 0.]\n",
      "1524 [ 0.] [ 0.]\n",
      "1525 [ 0.] [ 0.]\n",
      "1526 [ 0.] [ 0.]\n",
      "1527 [ 0.] [ 0.]\n",
      "1528 [ 0.] [ 0.]\n",
      "1529 [ 0.] [ 0.]\n",
      "1530 [ 0.] [ 0.]\n",
      "1531 [ 0.] [ 0.]\n",
      "1532 [ 0.] [ 0.]\n",
      "1533 [ 0.] [ 0.]\n",
      "1534 [ 0.] [ 0.]\n",
      "1535 [ 0.] [ 0.]\n",
      "1536 [ 0.] [ 0.]\n",
      "1537 [ 0.] [ 0.]\n",
      "1538 [ 0.] [ 0.]\n",
      "1539 [ 0.] [ 0.]\n",
      "1540 [ 0.] [ 0.]\n",
      "1541 [ 0.] [ 0.]\n",
      "1542 [ 0.] [ 0.]\n",
      "1543 [ 0.] [ 0.]\n",
      "1544 [ 0.] [ 0.]\n",
      "1545 [ 0.] [ 0.]\n",
      "1546 [ 0.] [ 0.]\n",
      "1547 [ 0.] [ 0.]\n",
      "1548 [ 0.] [ 0.]\n",
      "1549 [ 0.] [ 0.]\n",
      "1550 [ 0.] [ 0.]\n",
      "1551 [ 0.] [ 0.]\n",
      "1552 [ 0.] [ 0.]\n",
      "1553 [ 0.] [ 0.]\n",
      "1554 [ 0.] [ 0.]\n",
      "1555 [ 0.] [ 0.]\n",
      "1556 [ 0.] [ 0.]\n",
      "1557 [ 0.] [ 0.]\n",
      "1558 [ 0.] [ 0.]\n",
      "1559 [ 0.] [ 0.]\n",
      "1560 [ 0.] [ 0.]\n",
      "1561 [ 0.] [ 0.]\n",
      "1562 [ 0.] [ 0.]\n",
      "1563 [ 0.] [ 0.]\n",
      "1564 [ 0.] [ 0.]\n",
      "1565 [ 0.] [ 0.]\n",
      "1566 [ 0.] [ 0.]\n",
      "1567 [ 0.] [ 0.]\n",
      "1568 [ 0.] [ 0.]\n",
      "1569 [ 0.] [ 0.]\n",
      "1570 [ 0.] [ 0.]\n",
      "1571 [ 0.] [ 0.]\n",
      "1572 [ 0.] [ 0.]\n",
      "1573 [ 0.] [ 0.]\n",
      "1574 [ 0.] [ 0.]\n",
      "1575 [ 0.] [ 0.]\n",
      "1576 [ 0.] [ 0.]\n",
      "1577 [ 0.] [ 0.]\n",
      "1578 [ 0.] [ 0.]\n",
      "1579 [ 0.] [ 0.]\n",
      "1580 [ 0.] [ 0.]\n",
      "1581 [ 0.] [ 0.]\n",
      "1582 [ 0.] [ 0.]\n",
      "1583 [ 0.] [ 0.]\n",
      "1584 [ 0.] [ 0.]\n",
      "1585 [ 0.] [ 0.]\n",
      "1586 [ 0.] [ 0.]\n",
      "1587 [ 0.] [ 0.]\n",
      "1588 [ 0.] [ 0.]\n",
      "1589 [ 0.] [ 0.]\n",
      "1590 [ 0.] [ 0.]\n",
      "1591 [ 0.] [ 0.]\n",
      "1592 [ 0.] [ 0.]\n",
      "1593 [ 0.] [ 0.]\n",
      "1594 [ 0.] [ 0.]\n",
      "1595 [ 0.] [ 0.]\n",
      "1596 [ 0.] [ 0.]\n",
      "1597 [ 0.] [ 0.]\n",
      "1598 [ 0.] [ 0.]\n",
      "1599 [ 0.] [ 0.]\n"
     ]
    }
   ],
   "source": [
    "for i, in_vec in enumerate(IN.series.T):\n",
    "    print(i, in_vec, IN.series[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating the sparse random recurrent weight matrix WXX\n",
    "# as well as (input => RRN) and (RRN => output) weights\n",
    "GEN.initialize_weights()\n",
    "IN.initialize_weights(GEN)\n",
    "OUT.initialize_weights(GEN)\n",
    "\n",
    "# assigning them to workspace names\n",
    "WXX = GEN.wxx_ini\n",
    "WInputX = IN.winputx_ini\n",
    "WXOut = OUT.wxout_ini\n",
    "\n",
    "# intializing some arrays which will store histories\n",
    "# these are initalized OUTSIDE of the trial loop\n",
    "X_history = np.zeros((GEN.n_units, TRYAL.n_steps))\n",
    "Out_history = np.zeros((OUT.n_units, TRYAL.n_steps))\n",
    "\n",
    "# initializing some array which will store training statistics\n",
    "# these are initialized INSIDE of the trial loop\n",
    "WXOut_len = np.zeros((TRYAL.n_steps))\n",
    "WXX_len = np.zeros((TRYAL.n_steps))\n",
    "dW_readout_len = np.zeros((TRYAL.n_steps))\n",
    "dW_recurr_len = np.zeros((TRYAL.n_steps))\n",
    "\n",
    "# setting for this particular run\n",
    "TRAIN_RECURR = False\n",
    "TRAIN_READOUT = False\n",
    "use_noiseamp = 0\n",
    "time_div = GEN.tau_ms / TRYAL.time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:00<00:00, 1964.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating initial conditions for firing rate, activation level, and output\n",
    "Xv = 2 * np.random.rand(GEN.n_units, 1) - 1\n",
    "X = GEN.sigmoid(Xv)\n",
    "O = np.zeros((OUT.n_units,1))\n",
    "\n",
    "# creating all noise ahead of time\n",
    "all_noise = np.random.normal(scale=np.sqrt(TRYAL.time_step), size=(GEN.n_units, TRYAL.n_steps))\n",
    "\n",
    "train_window = 0\n",
    "for i in range(TRYAL.n_steps):\n",
    "\n",
    "    in_vec = IN.series[:, i]\n",
    "    noise = use_noiseamp * all_noise[:, [i]]\n",
    "    Xv_current = WXX * X + WInputX * in_vec + noise\n",
    "    Xv += (-Xv + Xv_current) / time_div\n",
    "    X = GEN.sigmoid(Xv)\n",
    "    O = np.dot(WXOut, X)\n",
    "    \n",
    "    # START SECTION FOR TRAINING IN RECURRENT / READOUT PARTS\n",
    "    if (i == TRYAL.start_train_n):\n",
    "        train_window = True\n",
    "    if (i == TRYAL.end_train_n):\n",
    "        train_window = False\n",
    "\n",
    "    if train_window and i % TRYAL.spacing == 0:\n",
    "\n",
    "        if TRAIN_RECURR:\n",
    "            error = X - Target_innate_X[:, i]\n",
    "            for plas in 1:GEN.n_plastic\n",
    "                \n",
    "        if TRAIN_READOUT:\n",
    "            pass\n",
    "    # END SECTION FOR TRAINING IN RECURRENT / READOUT PARTS\n",
    "\n",
    "    Out_history[:, i] = O\n",
    "    X_history[:, [i]] = X\n",
    "    WXOut_len[i] = np.sqrt(np.sum(np.square(WXOut)))\n",
    "    WXX_len[i] = np.sqrt(WXX.power(2).sum()) # note these are methods of sparse arrays\n",
    "#     WXX_len[i] = np.sqrt(np.sum(np.square(WXX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f84b0f3e940>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "\n",
    "ax1 = plt.subplot2grid((4, 1), (0, 0))\n",
    "ax2 = plt.subplot2grid((4, 1), (1, 0), rowspan=2)\n",
    "ax3 = plt.subplot2grid((4, 1), (3, 0))\n",
    "\n",
    "# top panel\n",
    "ax1.plot(TRYAL.time_ms, OUT.series.T, 'g')\n",
    "ax1.plot(TRYAL.time_ms, IN.series.T / 2, 'b')\n",
    "ax1.plot(TRYAL.time_ms, Out_history.T, 'r')\n",
    "\n",
    "# middle panel\n",
    "ten_trials = np.broadcast_to(np.expand_dims(TRYAL.time_ms, 1), (TRYAL.n_steps, 10))\n",
    "ten_traces = X_history[:10, :].T + np.arange(10)*2\n",
    "ax2.plot(ten_trials, ten_traces)\n",
    "\n",
    "# bottom panel\n",
    "ax3.plot(TRYAL.time_ms, WXOut_len)\n",
    "# ax3.plot(TRYAL.time_ms, WXX_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main loop\n",
    "def main():\n",
    "    # creating initial conditions for firing rate, activation level, and output\n",
    "    Xv = 2 * np.random.rand(GEN.n_units, 1) - 1\n",
    "    X = GEN.sigmoid(Xv)\n",
    "    O = np.zeros((OUT.n_units,1))\n",
    "    \n",
    "    # creating all noise ahead of time\n",
    "    all_noise = np.random.normal(scale=np.sqrt(TRYAL.time_step), size=(GEN.n_units, TRYAL.n_steps))\n",
    "    \n",
    "    train_window = 0\n",
    "    for i in tqdm(range(TRYAL.n_steps)):\n",
    "\n",
    "        in_vec = IN.series[:, i]\n",
    "        noise = use_noiseamp * all_noise[:, [i]]\n",
    "        Xv_current = WXX * X + WInputX * in_vec + noise\n",
    "        Xv += (-Xv + Xv_current) / time_div\n",
    "        X = GEN.sigmoid(Xv)\n",
    "        O = np.dot(WXOut, X)\n",
    "\n",
    "        if (i == TRYAL.start_train_n):\n",
    "            train_window = True\n",
    "        if (i == TRYAL.end_train_n):\n",
    "            train_window = False\n",
    "\n",
    "        if train_window and i % TRYAL.spacing == 0:\n",
    "\n",
    "            if TRAIN_RECURR:\n",
    "                error = X - Target_innate_X[:, i]\n",
    "                for plas in 1:GEN.n_plastic\n",
    "\n",
    "            if TRAIN_READOUT:\n",
    "                pass\n",
    "\n",
    "        Out_history[:, i] = O\n",
    "        X_history[:, [i]] = X\n",
    "        WXOut_len[i] = np.sqrt(np.sum(np.square(WXOut)))\n",
    "        # next line is the most expensive line\n",
    "        WXX_len[i] = np.sqrt(WXX.power(2).sum()) # convert to regular numpy array for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:00<00:00, 5100.82it/s]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1513.83it/s]\n"
     ]
    }
   ],
   "source": [
    "%lprun -f main main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw matlab code\n",
    "\n",
    "WXOut_len = np.zeros((1, TRYAL.n_steps))\n",
    "WXX_len = np.zeros((1, TRYAL.n_steps))\n",
    "dW_readout_len = np.zeros((1, TRYAL.n_steps))\n",
    "dW_recurr_len = np.zeros((1, TRYAL.n_steps))\n",
    "train_window = 0\n",
    "\n",
    "# initial conditions\n",
    "Xv = 1*(2*np.random.rand(numUnits,1)-1)\n",
    "X = GEN.sigmoid(Xv)\n",
    "Out = np.zeros(numOut,1)\n",
    "\n",
    "\n",
    "# integration loop\n",
    "for i = 1:n_steps\n",
    "\n",
    "    if rem(i,round(n_steps/10)) == 0 && (TRAIN_RECURR == 1 || TRAIN_READOUT == 1)\n",
    "        fprintf('.')\n",
    "    end\n",
    "\n",
    "    in_vec= input_pattern(:,i)\n",
    "\n",
    "    # update units\n",
    "    noise = use_noiseamp*np.random.normal(numUnits,1)*np.sqrt(TRYAL.time_step)\n",
    "    Xv_current = WXX*X + WInputX*in_vec+ noise\n",
    "    Xv = Xv + ((-Xv + Xv_current)./tau)*TRYAL.time_step\n",
    "    X = GEN.sigmoid(Xv)\n",
    "    Out = WXOut*X\n",
    "\n",
    "    # start-end training window\n",
    "    if (i == start_train_n)\n",
    "        train_window = 1\n",
    "    end\n",
    "    if (i == end_train_n)\n",
    "        train_window = 0\n",
    "    end\n",
    "\n",
    "    # training\n",
    "    if (train_window == 1 && rem(i,learn_every) == 0)\n",
    "\n",
    "        if TRAIN_RECURR == 1\n",
    "            # train recurrent\n",
    "            error = X - Target_innate_X(:,i)\n",
    "            for plas = 1:numplastic_Units\n",
    "                X_pre_plastic = X(pre_plastic_units(plas).inds)\n",
    "                P_recurr_old = P_recurr(plas).P\n",
    "                P_recurr_old_X = P_recurr_old*X_pre_plastic\n",
    "                den_recurr = 1 + X_pre_plastic'*P_recurr_old_X\n",
    "                P_recurr(plas).P = P_recurr_old - (P_recurr_old_X*P_recurr_old_X')/den_recurr\n",
    "                # update network matrix\n",
    "                dW_recurr = -error(plas)*(P_recurr_old_X/den_recurr)'\n",
    "                WXX(plas,pre_plastic_units(plas).inds) = WXX(plas,pre_plastic_units(plas).inds) + dW_recurr\n",
    "                # store change in weights\n",
    "                dW_recurr_len(i) = dW_recurr_len(i) + np.sqrt(dW_recurr*dW_recurr')\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if TRAIN_READOUT == 1\n",
    "            # update inverse correlation matrix (using property P' = P)\n",
    "            P_readout_old = P_readout\n",
    "            P_readout_old_X = P_readout_old*X\n",
    "            den_readout = 1 + X'*P_readout_old_X\n",
    "            P_readout = P_readout_old - (P_readout_old_X*P_readout_old_X')/den_readout\n",
    "            # update error\n",
    "            error = Out - target_Out(i)\n",
    "            # update output weights\n",
    "            dW_readout = -error*(P_readout_old_X/den_readout)'\n",
    "            WXOut = WXOut + dW_readout\n",
    "            # store change in weights\n",
    "            dW_readout_len(i) = np.sqrt(dW_readout*dW_readout')\n",
    "        end\n",
    "\n",
    "    end\n",
    "    # store output\n",
    "    Out_history(:,i) = Out\n",
    "    X_history(:,i) = X\n",
    "    WXOut_len(i) = np.sqrt(sum(reshape(WXOut.^2,numOut*numUnits,1)))\n",
    "    WXX_len(i) = np.sqrt(sum(reshape(WXX.^2,numUnits^2,1)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600,)\n",
      "(1, 1600)\n",
      "(1, 1600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x114726550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check: plot input and output patterns\n",
    "\n",
    "print(TRYAL.time_ms.shape)\n",
    "print(IN.series.shape)\n",
    "print(Out.series.shape)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(TRYAL.time_ms, IN.series.T, label='input')\n",
    "ax.plot(TRYAL.time_ms, Out.series.T, label='output')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick verification that all of the following ways of creating\n",
    "# a matrix containing random normal numbers\n",
    "# are equivalent\n",
    "\n",
    "prng = RandomState(1234) # note one needs to re-call this to get same results every time\n",
    "test1 = prng.normal(size=(GEN.n_units, GEN.n_units)) * GEN.scale_recurr\n",
    "\n",
    "prng = RandomState(1234)\n",
    "test2 = prng.normal(scale=GEN.scale_recurr, size=(GEN.n_units, GEN.n_units))\n",
    "\n",
    "prng = RandomState(1234)\n",
    "test3 = norm.rvs(scale=GEN.scale_recurr, size=(GEN.n_units, GEN.n_units), random_state=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm.pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_allcombs_equal((test1, test2, test3), ck_fun=assert_array_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing equality / speed of generating random sparse matrices\n",
    "\n",
    "# method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# uniform distribution for mask\n",
    "prng = RandomState(1234)\n",
    "WXX_mask = prng.rand(GEN.n_units, GEN.n_units)\n",
    "WXX_mask[WXX_mask <= GEN.p_connect] = 1\n",
    "WXX_mask[WXX_mask < 1] = 0\n",
    "\n",
    "# normal distribution for vals\n",
    "prng = RandomState(1234)\n",
    "WXX_vals = prng.normal(scale=GEN.scale_recurr, size=(GEN.n_units, GEN.n_units))\n",
    "\n",
    "# create non-sparse version of WXX and set self-connections (diagonal elements) to 0\n",
    "WXX_nonsparse = WXX_vals * WXX_mask\n",
    "np.fill_diagonal(WXX_nonsparse, 0)\n",
    "# WXX_nonsparse[np.diag_indices_from(WXX_nonsparse)] = 0\n",
    "\n",
    "# convert to be sparse\n",
    "# WXX = coo_matrix(WXX_nonsparse)\n",
    "WXX = csr_matrix(WXX_nonsparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rvs = norm(scale=GEN.scale_recurr).rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelseay/anaconda/lib/python3.5/site-packages/scipy/sparse/compressed.py:730: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "# random variable generator object\n",
    "prng = RandomState(1234)\n",
    "WXX_s = random(GEN.n_units, GEN.n_units, density=GEN.p_connect, random_state=prng, data_rvs=rvs,\n",
    "               format='csr',)\n",
    "#                format='csr',)\n",
    "# WXX_s[np.diag_indices_from(WXX_s)] = 0\n",
    "WXX_s.setdiag(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800) 63797 -0.775266142687 0.726642624784\n"
     ]
    }
   ],
   "source": [
    "cm = WXX_nonsparse\n",
    "print(cm.shape, np.count_nonzero(cm), cm.min(), cm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800) 63797 63797 -0.775266142687 0.726642624784\n",
      "(800, 800) 64723 63923 -0.742525396081 0.73124373277\n"
     ]
    }
   ],
   "source": [
    "ck_mats = (WXX, WXX_s)\n",
    "\n",
    "for cm in ck_mats:\n",
    "    print(cm.shape, cm.nnz, cm.count_nonzero(), cm.min(), cm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "imshow_cb(WXX.toarray(), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "imshow_cb(WXX_s.toarray(), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second method is slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ck_mats = (WXX_nonsparse, WXX, WXX_c, WXX_o, WInputX, WXOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing speed of *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 700 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test1_ns = WXX_nonsparse*X # SLOW, WRONG SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 60.6 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test1 = WXX*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 62.5 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test1_c = WXX_c*X # NO DIFFERENCE WITH ROW / COLUMN SPARSE MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 159 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test1_o = WXX_o*X # COO sparse matrix is slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800)\n",
      "(800, 1)\n",
      "(800, 1)\n",
      "(800, 1)\n"
     ]
    }
   ],
   "source": [
    "test1_ns = WXX_nonsparse*X # SLOW, WRONG SHAPE\n",
    "test1 = WXX*X\n",
    "test1_c = WXX_c*X # NO DIFFERENCE WITH ROW / COLUMN SPARSE MATRICES\n",
    "test1_o = WXX_o*X # COO sparse matrix is slower\n",
    "\n",
    "print(test1_ns.shape)\n",
    "print(test1.shape)\n",
    "print(test1_c.shape)\n",
    "print(test1_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ck_mats = (test1, test1_c, test1_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_allcombs_equal(ck_mats, assert_array_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing speed of dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.99 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 69.7 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test2_ns = WXX_nonsparse.dot(X) # ONLY VERY SLIGHTLY SLOWER THAN\n",
    "# SPARSE, CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 63.1 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test2 = WXX.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.52 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 62.4 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test2_c = WXX_c.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 160 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test2_o = WXX_o.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2_ns = WXX_nonsparse.dot(X) # ONLY VERY SLIGHTLY SLOWER THAN\n",
    "# SPARSE, CORRECT BUT NUMERICALLY SLIGHTLY DIFFERENT?\n",
    "test2 = WXX.dot(X)\n",
    "test2_c = WXX_c.dot(X)\n",
    "test2_o = WXX_o.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ck_mats = (test1, test1_c, test1_o, test2, test2_c, test2_o, test2_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ck_mats = (test2, test2_c, test2_o,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_allcombs_equal(ck_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_allcombs_equal(ck_mats, ck_fun=assert_array_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing speed of matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 7.55 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 69.5 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test3_ns = np.matmul(WXX_nonsparse, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object arrays are not currently supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-8f9de1d16825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test3 = np.matmul(WXX, X)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/michaelseay/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-58>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/Users/michaelseay/anaconda/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelseay/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0mworst_tuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworst_tuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelseay/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object arrays are not currently supported"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test3 = np.matmul(WXX, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3_ns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conclusion, for multiplying the sparse matrix by a vector, it's fastest\n",
    "# for a csr_matrix and just use the * operator, it will do the matrix\n",
    "# multiplication in about 60 us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing 3\n",
    "\n",
    "fastest way to square, sum, and square root a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 26.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.sqrt(np.sum(np.square(WXX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.sqrt(np.sum(np.square(WXX.toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.97 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 263 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.sqrt(WXX.power(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = np.sqrt(np.sum(np.square(WXX)))\n",
    "t2 = np.sqrt(np.sum(np.square(WXX.toarray())))\n",
    "t3 = np.sqrt(WXX.power(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conclusion: use method 3"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
